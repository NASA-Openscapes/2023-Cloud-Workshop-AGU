{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d00560f9-1def-4210-b97c-34de75e49f0a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"NASA Earthdata Cloud Clinic\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf218d52-4c89-48a1-bbb0-567e2378e0db",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Welcome to the NASA Earthdata Cloud Clinic. \n",
    "\n",
    "We will go through **two different direct cloud access & subsetting options available in the Earthdata Cloud:** \n",
    "\n",
    "1. **The [earthaccess](https://github.com/nsidc/earthaccess) python library** for data search and direct cloud access, followed by `xarray` subsetting \n",
    "    * Working with [MEaSUREs Sea Surface Height Anomalies](https://podaac.jpl.nasa.gov/dataset/SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205)\n",
    "    * Discover data using [Earthdata Search](https://search.earthdata.nasa.gov/search/granules?p=C2270392799-POCLOUD&pg[0][v]=f&pg[0][gsk]=-start_date&q=measures%20ssh%20anomalies&tl=1685392107!3!!)\n",
    "2. **The [Harmony-py](https://github.com/nasa/harmony-py) python library** for direct cloud access & data subsetting\n",
    "    * Requesting a subset of data from the [GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (v4.1)](https://podaac.jpl.nasa.gov/dataset/MUR-JPL-L4-GLOB-v4.1) dataset using a vector-based geospatial file.\n",
    "    * This dataset can also be viewed in [Earthdata Search](https://search.earthdata.nasa.gov/search?q=GHRSST%20Level%204%20MUR%20v4.1&ff=Available%20in%20Earthdata%20Cloud!Customizable). \n",
    "    \n",
    "In both scenarios, we will be accessing data directly from Amazon Web Services (AWS), specifically in the us-west-2 region, which is where all cloud-hosted NASA Earthdata reside. This shared compute environment (JupyterHub) is also running in the same location. We will then load the data into Python as an `xarray` `dataset`.\n",
    "\n",
    "See the bottom of the notebook for additional resources, including several tutorials that that served as a foundation for this clinic. \n",
    "\n",
    "### A note on subsetting\n",
    "\n",
    "In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection/regridding, and file aggregation. What does subsetting mean? Here's a generalized graphic of what we mean. \n",
    "\n",
    "![](images/subsetting_diagram.png){fig-alt=\"Three maps of the United States are present, with a red bounding box over the state of Colorado. Filtering and subsetting are demonstrated by overlaying SMAP L2 data, with data overlapping and cropping the rectangle, respectively.\"  width=60%}\n",
    "\n",
    "Note: \"direct cloud access\" is also called \"direct S3 access\" or simply \"direct access\". And \"subsetting\" is also called \"transformation\".\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Utilize the `earthaccess` python library to search for data using spatial and temporal filters and explore search results\n",
    "2. Perform in-region direct access of data from an Amazon Simple Storage Service (S3) bucket\n",
    "3. Extract variables and spatial slices from an `xarray` dataset \n",
    "4. Plot data using `xarray` \n",
    "5. Conceptualize data subsetting services provided by NASA Earthdata, including Harmony\n",
    "6. Plot a polygon geojson file with a basemap using `geoviews` \n",
    "7. Utilize the `harmony-py` library to request data over the Gulf of Mexico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b962cdc1-5bfc-42ad-b522-e870822f9540",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488de1c-9177-405a-b957-92045aad16ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "First we'll import python packages and set our authentication that will be used for both of our access and subsetting methods.\n",
    "\n",
    "\n",
    "You'll also need to be aware that data in NASA's Earthdata Cloud reside in Amazon Web Services (AWS) Simple Storage Service (S3) buckets. Access is provided via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region. While this compute location is required for direct S3 access, all data in Earthdata Cloud are still freely available via download.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35fcbf7-37cc-477e-be92-3d767e06dbe4",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6fad23-fa66-4e3d-818b-96f797025b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Direct access\n",
    "import earthaccess \n",
    "from pprint import pprint\n",
    "import xarray as xr\n",
    "\n",
    "# Harmony\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "gv.extension('bokeh', 'matplotlib')\n",
    "from harmony import BBox, Client, Collection, Request, LinkType\n",
    "import datetime as dt\n",
    "import s3fs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736215c-fec1-4171-81ae-98c33e6eb732",
   "metadata": {},
   "source": [
    "### About `earthaccess`  \n",
    "\n",
    "`earthaccess` leverages the [Common Metadata Repository (CMR) API](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html) to search for collections and granules.  \n",
    "\n",
    "1. Authentication\n",
    "\n",
    "2. Search\n",
    "\n",
    "Transition from search results to `xarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8a614-de01-4905-949c-17588d9a623f",
   "metadata": {},
   "source": [
    "### Authentication for NASA Earthdata \n",
    "\n",
    "An Earthdata Login account is required to access data from NASA Earthdata. Please visit <https://urs.earthdata.nasa.gov> to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n",
    "\n",
    "The first step is to get the correct authentication that will allow us to get cloud-hosted data from NASA. This is all done through Earthdata Login. We can use the `earthaccess` library here, where the `login` method also gets the correct AWS credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938cefcd-63e5-4118-8500-45e74d6dfc89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auth = earthaccess.login(strategy = 'interactive', \n",
    "                         persist = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab966f-d1c8-4293-9d26-82e8ecb53b8c",
   "metadata": {},
   "source": [
    "### Search for data\n",
    "\n",
    "Earthdata Search also uses the CMR API. Let's head back to our [Earthdata Search](https://search.earthdata.nasa.gov/search/granules?p=C2270392799-POCLOUD&pg[0][v]=f&pg[0][gsk]=-start_date&q=measures%20ssh%20anomalies&tl=1685392107!3!!) results to gather more information about our dataset of interest. The dataset \"short name\" can be found by clicking on the Info button on our collection search result, and we can paste that into a python variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684ea9f-6a9e-4cda-af44-194bd462cf2b",
   "metadata": {},
   "source": [
    "| Shortname | Collection Concept ID | DOI |\n",
    "| --- | --- | --- |\n",
    "| GPM_3IMERGDF | C2723754864-GES_DISC | 10.5067/GPM/IMERGDF/DAY/07 |\n",
    "| MOD10C1 | C1646609808-NSIDC_ECS | 10.5067/MODIS/MOD10C1.061 |\n",
    "| SPL4SMGP | C2531308461-NSIDC_ECS | 10.5067/EVKPQZ4AFC4D | \n",
    "| SPL4SMAU | C2537927247-NSIDC_ECS | 10.5067/LWJ6TF5SZRG3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd0392-dcd9-46f8-96b9-3a1552bb3742",
   "metadata": {},
   "source": [
    "#### Search by collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2a5cd-940a-4206-8944-728d48995094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_id = 'C2723754864-GES_DISC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d096dc-c8ee-44d3-a130-b1dc5d9ae3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    concept_id = collection_id,\n",
    "    cloud_hosted = True,\n",
    "    count = 10    # Restricting to 10 records returned\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db512c-9f43-4da7-ae79-562f0ddbd1eb",
   "metadata": {},
   "source": [
    "In this example we used the `concept_id` parameter to search from our desired collection. However, there are multiple ways to specify the collection(s) we are interested in. Alternative parameters include:  \n",
    "\n",
    "- `doi` - request collection by digital object indentifier (e.g., `doi` = '10.5067/GPM/IMERGDF/DAY/07')  \n",
    "- `short_name` - request collection by CMR shortname (e.g., `short_name` = 'GPM_3IMERGDF')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd840e8d-02e4-49e0-857b-ef2021ae7858",
   "metadata": {},
   "source": [
    "We can refine our search by passing more parameters that describe the spatiotemporal domain of our use case. Here, we use the `temporal` parameter to request a date range and the `bounding_box` parameter to request granules that intersect with a bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5965f5-6cad-4a59-849e-612634ee646e",
   "metadata": {},
   "source": [
    "For our bounding box, we are going to read in a GeoJSON file containing a single feature and extract the coordinate pairs for the southeast corner and the northwest corner (or lowerleft and upperright corners) of the bounding box around the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657b16d-b7f1-4e7a-a196-0c2209d5d7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inGeojson = gpd.read_file('../../2023-Cloud-Workshop-AGU/data/sf_to_sierranvmt.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55396853-47ef-4a66-9181-cb75b16810d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xmin, ymin, xmax, ymax = inGeojson.total_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24277057-6726-4c36-8d52-096b29a7931e",
   "metadata": {},
   "source": [
    "We will assign our start date and end date to a varialbe named `date_range` and we'll assign the southeast and the northwest corner coordinates to a variable named `bbox` to be passed to our `earthaccess` search request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4bf4d-45d0-4bea-a910-4e065f670f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_range = (\"2022-11-19\", \"2023-04-06\")\n",
    "bbox = (-127.0761, 31.6444, -113.9039, 42.6310)\n",
    "#bbox = (xmin, ymin, xmax, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521d54f-47f9-48ee-9773-b2853495e707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    concept_id = collection_id,\n",
    "    #cloud_hosted = True,\n",
    "    temporal = date_range,\n",
    "    bounding_box = bbox,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12ea81-3596-4b59-bd95-5767409793f8",
   "metadata": {},
   "source": [
    "- The `short_name` and `concept_id` search parameters can be used to request one or multiple collections per request, but the `doi` parameter can only request a single collection.  \n",
    "> `concept_ids` = ['C2723754864-GES_DISC', 'C1646609808-NSIDC_ECS']  \n",
    "- CMR concept IDs and collection DOIs are unique to each version of a data collection. However CMR shortnames are not. CMR shortnames can be associated with multiple versions of a collection, so it is recommended to use the `short_name` parameter and the `version` parameter in conjuction.  \n",
    "- Use the `cloud_hosted` search parameter only to search for data assets available from NASA's Earthdata Cloud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b159c3-6bce-4d99-989b-503ac4231703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# col_ids = ['C2723754864-GES_DISC', 'C1646609808-NSIDC_ECS', 'C2531308461-NSIDC_ECS', 'C2537927247-NSIDC_ECS'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13477263-a7f6-4fc7-8c55-e9a4cac9f948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = earthaccess.search_data(\n",
    "#     concept_id = col_ids,\n",
    "#     #cloud_hosted = True,\n",
    "#     temporal = date_range,\n",
    "#     bounding_box = bbox,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99c18d-b5d4-4c29-a18d-646cea4b5ac5",
   "metadata": {},
   "source": [
    "## Working with `earthaccess` returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32ed97-06d9-4c12-8293-c85743977f9a",
   "metadata": {},
   "source": [
    "`earthaccess` provides several convienence methods to help streamline processes that historically have be painful when done using traditional methods. Following the search for data, you'll likely take one of two pathways with those results. You may choose to **download** the assets that have been returned to you or you may choose to continue working with the search results within the Python environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552d778-3fa8-4cdc-ac84-d219b51eb7e6",
   "metadata": {},
   "source": [
    "### Download `earthaccess` results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e94e17-bd06-4531-9d58-35a19f5f52c5",
   "metadata": {},
   "source": [
    "`earthaccess` makes downloading the data from the search results very easy using the `earthaccess.download()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5127f-843a-4ab3-8bc1-4eb815f19056",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_files = earthaccess.download(\n",
    "    results[0:9],\n",
    "    local_path='../../2023-Cloud-Workshop-AGU/data',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87287b-2741-4e68-a4f1-675ffc2ce21a",
   "metadata": {},
   "source": [
    "### Explore `earthaccess` search response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad180d-ec74-4245-bb41-ec745912b5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'The results variable is a {type(results)} of {type(results[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271ada5-e4b3-4397-be59-4ef6a1613576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8ee09-fccd-4938-9d7c-775292600ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item = results[0]\n",
    "type(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362bef8-48c8-4a08-82a3-0f97c5aea2ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item['meta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b5eb7-7325-4f30-9620-5bc34f3ecedf",
   "metadata": {},
   "source": [
    "#### Get data URLs / S3 URIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8951d85-762d-498f-9c70-e5788a1ce2ea",
   "metadata": {},
   "source": [
    "Get links to data. The `data_links()` method is used to return the URL(s)/data link(s) for the item. By defalt the method returns the HTTPS URL to download or access the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ef01c-ba42-475e-9ee0-c47d25b4dc06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item.data_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6a806-4801-4b58-b51c-e6ce29c9c9a0",
   "metadata": {},
   "source": [
    "The `data_links()` method can also be used to get the s3 URI when we want to perform direct s3 access of the data in the cloud. To get the s3 URI, pass `access = 'direct'` to the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35871bd-6d09-4459-beac-88c899f0af55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item.data_links(access='direct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d585f0-9739-48a7-9bef-a5ee1c45b0d3",
   "metadata": {},
   "source": [
    "Finally, we can extract all of the data links from our search results and add them to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4cc751-a62e-4515-8bbc-1042aadefae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_link_list = []\n",
    "\n",
    "for granule in results:\n",
    "    for asset in granule.data_links(access='direct'):\n",
    "        data_link_list.append(asset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c149a0-8766-4881-a82d-56c855e552ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_link_list[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae20ce8-6b9a-4b70-abd1-80607bb5b3fd",
   "metadata": {},
   "source": [
    "#### Pass results to `xarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8f9a3-2cb5-4f41-8e78-ddcaab2506cf",
   "metadata": {},
   "source": [
    "We pass the `earthaccess.open()` function to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e98b1-3a65-48a8-a68e-c5cbad4420bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(earthaccess.open(results), chunks = {})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87bdda-7dc8-458a-ab99-50aca9ed41c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.precipitation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1edf22-8fa4-4907-9aa8-fbd143fa4252",
   "metadata": {},
   "source": [
    "Direct S3 access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c6389-f438-4cc7-829a-b2aeb3632de1",
   "metadata": {},
   "source": [
    "To leverage direct S3 access in AWS us-west-2 we have to request temporary S3 credentials. `earthaccess` has a convience function for that too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84f057-ab30-4c3f-9bf7-0dd802ab7202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_endpoint = results[0].get_s3_credentials_endpoint()\n",
    "s3_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938545d-7137-47eb-8851-67b240ff16e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_credentials = auth.get_s3_credentials(endpoint = s3_endpoint)\n",
    "#s3_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee521272-8a26-466c-8a81-1a674dd65ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs_s3 = s3fs.S3FileSystem(anon=False, key=s3_credentials['accessKeyId'], secret=s3_credentials['secretAccessKey'], token=s3_credentials['sessionToken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3217b86-fbfb-4946-ae71-f2c77b482bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileset = earthaccess.open(data_link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc375707-ae69-4a90-9c64-f3bce8cb09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3_file_obj = fs_s3.open(ssh_s3, mode='rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e45b28-b287-41c6-acd0-2ed6586ca9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(fileset, engine='h5netcdf')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295dee03-36c3-4e9f-8031-bc572391345b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98274ec4-656d-4a7a-be96-c6184a2df0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bd0d2-bb9e-4375-842d-45633b11350f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8bab8-cb94-4d82-a3c1-ca97e505bbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9668c092-ed2e-4879-8827-72ad01088cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    \"concept_id\": \"C2408009906-LPCLOUD\", # CMR concept ID for EMITL1BRAD.001\n",
    "    #\"day_night_flag\": \"day\",\n",
    "    \"cloud_cover\": (0, 10),\n",
    "    \"temporal\": (\"2022-05\", \"2023-08\"),\n",
    "    \"bounding_box\": (-99.65, 18.85, -98.5, 19.95)\n",
    "}\n",
    "results = earthaccess.search_data(**search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5189ea-d6de-4a1c-8e78-37f2771c0e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = results[0]\n",
    "d.data_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200e0ac-f0a8-45e0-9f26-cd9b90199076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[x.data_links() for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df92ef-4eec-4c08-96a2-9fdcbc73fbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6a109-41c8-443b-9fd1-5ccf7678d919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b312a4-390e-4158-8127-d043e17af457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "827f9971-a344-4a18-b3fc-8ef250be99e4",
   "metadata": {},
   "source": [
    "### Access data\n",
    "\n",
    "Our code will work the same way if we are running it \"in-region\", within our shared cloud environment, or locally from our laptop.\n",
    "\n",
    "Since we are working in the AWS `us-west-2` region, we can stream data directly to xarray. We are using the `open_mfdataset()` (multi-file) method, which is required when using `earthaccess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e178e1-cea1-497a-942d-05091a2e82b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(earthaccess.open(results))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b73b1-ea96-4a36-883d-7b644181f5ea",
   "metadata": {},
   "source": [
    "### Subset data \n",
    "We can create a subsetted xarray dataset by extracting the SLA variable and slicing the dataset by a smaller area of interest near the state of Baja, Mexico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657bbde-c9a9-42f1-ae76-c8dd7724b354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_subset = ds['SLA'].sel(Latitude=slice(15.8, 35.9), Longitude=slice(234.5,260.5)) \n",
    "ds_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2118d08-e9e5-4bd1-9363-01ab45129a51",
   "metadata": {},
   "source": [
    "Use the built-in plotting function of xarray to create a plot of SLA standard deviation over time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70889735-5e52-4f79-a210-c1dbc95e4a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_subset.std('Time').plot(figsize=(10,6), x='Longitude', y='Latitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf7721-3a70-4dab-ac66-16fcf3fc8cf6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b8d37-cc33-48ec-9145-a702dde0ee79",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. [`harmony-py`](https://github.com/nasa/harmony-py) + [`xarray`](https://docs.xarray.dev/en/stable/) \n",
    "\n",
    "We will be accessing a subsetted NetCDF-4 file using Transformation Services in the Cloud. \n",
    "\n",
    "Using [Earthdata Search](https://search.earthdata.nasa.gov/search?q=GHRSST%20Level%204%20MUR%20v4.1&ff=Available%20in%20Earthdata%20Cloud!Customizable), we can find datasets that support these services using the \"Customizable\" filter. \n",
    "\n",
    "We will find, request, and open customized data using [Harmony](https://harmony.earthdata.nasa.gov/), below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca030c-38a0-4118-a317-7a74f62cae60",
   "metadata": {},
   "source": [
    "### Define area of interest\n",
    "\n",
    "First, use `geopandas` to read in a geojson file containing a polygon feature over the Gulf of Mexico. The geojson file is found in the ~/data directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be657f21-bf7a-4fef-b2d7-4141a3cd5de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geojson_path = './data/gulf.json'\n",
    "gdf = gpd.read_file(geojson_path) #Return a GeoDataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097684e8-40df-4c22-ba28-91c3499155b9",
   "metadata": {},
   "source": [
    "We can plot the polygon using the `geoviews` package that we imported as gv with ‘bokeh’ and ‘matplotlib’ extensions. The following has reasonable width, height, color, and line widths to view our polygon when it is overlayed on a base tile map. We can view this with the `*` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa980572-a8f2-4324-93bf-c3e4f7102a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base = gv.tile_sources.EsriImagery.opts(width=650, height=500)\n",
    "ocean_map = gv.Polygons(gdf).opts(line_color='yellow', line_width=5, color=None)\n",
    "base * ocean_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce6fc6-1f2c-45a0-a824-c67f29a83e35",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a subset request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d05f77-8c21-4c7e-b865-b191398f93ae",
   "metadata": {},
   "source": [
    "Here we'll create a request for a spatial subset of data.\n",
    "\n",
    "First, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\n",
    "\n",
    "When creating the Client, we need to provide Earthdata Login credentials. This basic line below assumes that we have a `.netrc` available. See the Earthdata Cloud Cookbook [appendix](\"https://nasa-openscapes.github.io/earthdata-cloud-cookbook/appendix/authentication.html\") for more information on [Earthdata Login](\"https://urs.earthdata.nasa.gov/\") and netrc setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387b657-10aa-4503-8454-3a7438734dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "harmony_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f5aef-8427-4ae7-8238-8c8adee8347d",
   "metadata": {},
   "source": [
    "See the [harmony-py documentation](https://harmony-py.readthedocs.io/en/latest/) for details on how to construct your request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f1aa6-3b71-4b6f-a321-3c9d7eef8234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sst_short_name=\"MUR-JPL-L4-GLOB-v4.1\"\n",
    "\n",
    "request = Request(\n",
    "    collection=Collection(id=sst_short_name),\n",
    "    shape=geojson_path,\n",
    "    temporal={\n",
    "    'start': dt.datetime(2021, 8, 1, 1),\n",
    "    'stop': dt.datetime(2021, 8, 1, 2)   \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc83c1-a2ea-41bb-b319-48fde922bf8d",
   "metadata": {},
   "source": [
    "### Submit a subset request\n",
    "\n",
    "Now that the request is created, we can now submit it to Harmony using the Harmony Client object. A job id is returned, which is a unique identifier that represents the submitted request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe541923-f17e-4027-831b-e35004b1c36f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_id = harmony_client.submit(request)\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e74e62-aacd-4dff-ac45-5e9f322f0677",
   "metadata": {},
   "source": [
    "### Check request status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a49cb5-6e27-42c6-83a0-0d88f50e9fb8",
   "metadata": {},
   "source": [
    "Depending on the size of the request, it may be helpful to wait until the request has completed processing before the remainder of the code is executed. The wait_for_processing() method will block subsequent lines of code while optionally showing a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414eaaa-0b84-4023-ab72-d49c068c1470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "harmony_client.wait_for_processing(job_id, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17ff08-1a87-439e-8399-db24b1844c7d",
   "metadata": {},
   "source": [
    "We can view Harmony job response and output URLs\n",
    "Once the data request has finished processing, we can view details on the job that was submitted to Harmony, including the API call to Harmony, and informational messages on the request if available.\n",
    "\n",
    "`result_json()` calls `wait_for_processing()` and returns the complete job in JSON format once processing is complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee7ccf-fc54-4f07-9223-f724d968c3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = harmony_client.result_json(job_id)\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7948bd-433e-428c-a4c0-a9651e8775dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Access data\n",
    "\n",
    "Just like above, the subsetted outputs produced by Harmony can be accessed directly from the cloud. \n",
    "\n",
    "First, we'll retrieve list of output URLs.\n",
    "\n",
    "The `result_urls()` method calls `wait_for_processing()` and returns a list of the processed data URLs once processing is complete. You may optionally show the progress bar as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924317f8-74de-4937-bf91-1e4e408ca9f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = harmony_client.result_urls(job_id, link_type=LinkType.s3)\n",
    "urls = list(results)\n",
    "url = urls[0]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b62050-88ad-4c50-9d38-6fabf36dc612",
   "metadata": {},
   "source": [
    "Using `aws_credentials` you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86419068-0d82-4cd1-84d1-eb59b699e07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "creds = harmony_client.aws_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a10c1-f72c-46f5-9b22-3b3eabb7b179",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Open staged files with *s3fs* and *xarray*\n",
    "\n",
    "We use the AWS `s3fs` package to create a file system that can then be read by xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3eb41d-d059-4ffb-b2b3-8675f238e874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_fs = s3fs.S3FileSystem(\n",
    "    key=creds['aws_access_key_id'],\n",
    "    secret=creds['aws_secret_access_key'],\n",
    "    token=creds['aws_session_token'],\n",
    "    client_kwargs={'region_name':'us-west-2'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b411be-49be-47a5-bad2-e3cceec250fd",
   "metadata": {},
   "source": [
    "Now that we have our s3 file system set, including our declared credentials, we'll use that to open the url, and read in the file through xarray. This extra step is needed because xarray cannot open the S3 location directly. Instead, the S3 file object is passed to xarray, in order to then open the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef6817-ee65-4fd4-9f66-85e51c8102c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = s3_fs.open(url, mode='rb')\n",
    "ds = xr.open_dataset(f)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb218932-4896-4902-8259-054dd10e4ad9",
   "metadata": {},
   "source": [
    "As before, we use the xarray built in plotting function to create a simple plot along the x and y dimensions of the dataset. We can see that the data are subsetted to our polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb575963-f7f6-47e8-b7a3-a540c50aa03c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.analysed_sst.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f0870-fe10-4df1-8107-bd8591d86d57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Tutorials\n",
    "\n",
    "This clinic was based off of several notebook tutorials including those presented during [past workshop events](https://nasa-openscapes.github.io/earthdata-cloud-cookbook/tutorials/), along with other materials co-created by the NASA Openscapes mentors:\n",
    "* [2021 Earthdata Cloud Hackathon](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/)\n",
    "* [2021 AGU Workshop](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/)\n",
    "* [Accessing and working with ICESat-2 data in the cloud](https://github.com/nsidc/NSIDC-Data-Tutorials/tree/main/notebooks/ICESat-2_Cloud_Access)\n",
    "* [Analyzing Sea Level Rise Using Earth Data in the Cloud](https://github.com/betolink/earthaccess-gallery/blob/main/notebooks/Sea_Level_Rise/SSL.ipynb)\n",
    "\n",
    "### Cloud services\n",
    "\n",
    "The examples used in the clinic provide an abbreviated and simplified workflow to explore access and subsetting options available through the Earthdata Cloud. There are several other options that can be used to interact with data in the Earthdata Cloud including: \n",
    "\n",
    "* [OPeNDAP](https://opendap.earthdata.nasa.gov/) \n",
    "    * Hyrax provides direct access to subsetting of NASA data using Python or your favorite analysis tool\n",
    "    * Tutorial highlighting OPeNDAP usage: https://nasa-openscapes.github.io/earthdata-cloud-cookbook/how-tos/working-locally/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html\n",
    "* [Zarr-EOSDIS-Store](https://github.com/nasa/zarr-eosdis-store)\n",
    "    * The zarr-eosdis-store library allows NASA EOSDIS Collections to be accessed efficiently by the Zarr Python library, provided they have a sidecar DMR++ metadata file generated. \n",
    "    * Tutorial highlighting this library's usage: https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/09_Zarr_Access.html \n",
    "\n",
    "### Support\n",
    "\n",
    "* [Earthdata Forum](https://forum.earthdata.nasa.gov/)\n",
    "    * User Services and community support for all things NASA Earthdata, including Earthdata Cloud\n",
    "* [Earthdata Webinar series](https://www.earthdata.nasa.gov/learn/webinars-and-tutorials)\n",
    "    * Webinars from DAACs and other groups across EOSDIS including guidance on working with Earthdata Cloud\n",
    "    * See the [Earthdata YouTube channel](https://www.youtube.com/@NASAEarthdata/featured) for more videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe97e1a-b779-42d6-8420-c1cc029b5334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
