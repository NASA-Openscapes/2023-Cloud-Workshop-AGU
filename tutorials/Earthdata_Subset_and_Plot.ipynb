{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bcf23fc8-ae63-4efe-84bb-c67acdffb973",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data subsetting and plotting with earthaccess, xarray, and harmony\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d9430-1a3e-480c-bf15-c35f938b4210",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In these examples we will use the [xarray](https://xarray.dev/), [earthaccess](https://nsidc.github.io/earthaccess/), and [harmony-py](https://github.com/nasa/harmony-py) libraries to subset data and make figures using `cartopy`, `matplotlib`, and `geoviews`.\n",
    "\n",
    "We will go through **three examples of subsetting and plotting data in the Earthdata Cloud:** \n",
    "\n",
    "1. Example 1 - Precipitation estimates from [IMERG, Daily Level 3 data](https://doi.org/10.5067/GPM/IMERGDF/DAY/07)\n",
    "2. Example 2 - Snow cover data from [MODIS/Terra, Daily Level 3 data](https://doi.org/10.5067/MODIS/MOD10C1.061)\n",
    "4. Example 3 - `harmony-py` for direct cloud access & subsetting of precipitable water data from the [DSCOVR EPIC Composite](https://doi.org/10.5067/EPIC/DSCOVR/L2_COMPOSITE_01).\n",
    "3. Appendix - Another example of `xarray` subsetting & plotting with Snow mass data from [SMAP, 3-hourly Level 4 data](https://doi.org/10.5067/EVKPQZ4AFC4D)\n",
    "    \n",
    "In each example, we will be accessing data directly from Amazon Web Services (AWS), specifically in the us-west-2 region, which is where all cloud-hosted NASA Earthdata reside. This shared compute environment (JupyterHub) is also running in the same location. We will then load the data into Python as an `xarray` dataset.\n",
    "\n",
    "For `harmony-py`, we will demonstrate an example of pulling data via the cloud from an existing on-premise data server.\n",
    "\n",
    "See the bottom of the notebook for additional resources, including several tutorials that that served as a foundation for this clinic. Includes: https://github.com/rupesh2/atmospheric_rivers/tree/main\n",
    "\n",
    "Note: \"direct cloud access\" is also called \"direct S3 access\" or simply \"direct access\".\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Extract variables, temporal slices, and spatial slices from an `xarray` dataset \n",
    "2. Plot data and exclude data points via boolean conditions, using `xarray`, `cartopy`, `matplotlib`, and `rasterio`\n",
    "3. Plot a polygon geojson file with a basemap using `geoviews` \n",
    "4. Conceptualize data subsetting services provided by NASA Earthdata, including Harmony\n",
    "5. Utilize the `harmony-py` library to request data over the Bay of San Francisco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d78efc-2d62-428a-a813-e58f949ee1bf",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a653d-b9e5-4cfe-a198-b5b612389742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "from pprint import pprint\n",
    "\n",
    "# Direct access\n",
    "import earthaccess\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "xr.set_options(display_expand_attrs=False)\n",
    "\n",
    "# Harmony\n",
    "from harmony import BBox, Client, Collection, Request, LinkType, CapabilitiesRequest\n",
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "gv.extension('bokeh', 'matplotlib', logo=False)\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LatitudeFormatter, LongitudeFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rxr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442bd92a-8f2d-4448-a59e-da4567710730",
   "metadata": {},
   "source": [
    "## Picking up where we left off\n",
    "\n",
    "We will authenticate our Earthaccess session, and then open the results like we did in the Search & Discovery section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0002f-c759-4611-8dd7-861b8bd38971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a3cb10-6988-401e-a618-59e2f5ac3228",
   "metadata": {},
   "source": [
    "## Example 1 - Xarray Subsetting - Precipitation estimates from IMERG, Daily Level 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b794c8-a100-46f0-8020-e2341ff2b201",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset\n",
    "We will use the GPM IMERG Final Precipitation L3 Daily dataset for this tutorial.  The IMERG Precipitation Rate provides the rain and snow rates in millimeters per hour (mm/hr). It is estimated by the Integrated Multi-satellitE Retrievals for Global Precipitation Measurement (GPM) (IMERG) algorithm. The IMERG algorithm uses passive-microwave data from the GPM constellation of satellites and infrared data from geosynchronous satellites. IMERG “morphs” observations to earlier or later times using wind from weather-model analyses. The daily IMERG dataset is derived from the half-hourly GPM_3IMERGHH. The derived result represents the final estimate of the daily mean precipitation rate in mm/day.\n",
    "\n",
    "The IMERG data has 0.1 x 0.1 degree latitude-longitude resolution (approximately 11 by 11 km at the Equator). The grid covers the globe, although precipitation cannot always be estimated near the Poles. The dataset and algorithm are described in the [data user guide](https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/doc/README.GPM.pdf) and the [Algorithm Theoretical Basis Document (ATBD)](https://arthurhou.pps.eosdis.nasa.gov/Documents/IMERG_V07_ATBD_final.pdf). \n",
    "\n",
    "Please cite the dataset as:\n",
    "> Huffman, G.J., E.F. Stocker, D.T. Bolvin, E.J. Nelkin, Jackson Tan (2023), GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 degree V07, Edited by Andrey Savtchenko, Greenbelt, MD, Goddard Earth Sciences Data and Information Services Center (GES DISC), https://doi.org/10.5067/GPM/IMERGDF/DAY/07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe9828-37e9-4949-846f-297057e5b0d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_id = 'C2723754864-GES_DISC'  # GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 degree V07 (GPM_3IMERGDF)\n",
    "\n",
    "# Bounds within which we search for data granules\n",
    "date_start = \"2023-02-24\"\n",
    "date_end = \"2023-02-26\"\n",
    "date_range = (date_start, date_end)\n",
    "bbox = (-127.0761, 31.6444, -113.9039, 42.6310)\n",
    "\n",
    "# For reference, here is a GeoJSON representing the above bounding box:\n",
    "# {\"type\": \"FeatureCollection\", \"features\": [{\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"LineString\", \"bbox\": [-127.0761, 31.6444, -113.9039, 42.631], \"coordinates\": [[-113.9039, 42.631], [-127.0761,42.631], [-127.0761, 31.6444], [-113.9039, 31.6444], [-113.9039, 42.631]]}}]}\n",
    "\n",
    "results = earthaccess.search_data(\n",
    "    concept_id = collection_id,\n",
    "    #cloud_hosted = True,\n",
    "    temporal = date_range,\n",
    "    bounding_box = bbox,\n",
    ")\n",
    "\n",
    "ds = xr.open_mfdataset(earthaccess.open(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72eb32d-8421-4f54-a2bd-7b8bc3dc531a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's print out all the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8beea9-bc30-4d02-9401-5b8605ad6847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for v in ds.variables:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e376f-149f-4d5f-aa2e-f292e951170f",
   "metadata": {},
   "source": [
    "Of the variables listed above, we are interested in three variables: `precipitation`, `precipitation_cnt_cond`, and `probabilityLiquidPrecipitation`. Let's print their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0241c11-ac86-418a-92e4-bb289468cc16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.variables['precipitation'].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91434dd4-8aad-4b95-9ee4-b5dce04c60a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.variables['precipitation_cnt_cond'].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f9e55-aa5e-4c4a-a571-2f29917e360e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.variables['probabilityLiquidPrecipitation'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480119f-777f-42b6-ae2c-d9a5de1cb8a0",
   "metadata": {},
   "source": [
    "### Subsetting\n",
    "\n",
    "In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection/regridding, and file aggregation. What does subsetting mean? To **subset** means to extract only the portions of a dataset that are needed for a given purpose. Here's a generalized graphic of what we mean. \n",
    "\n",
    "![](https://github.com/NASA-Openscapes/earthdata-cloud-cookbook/blob/main/examples/images/subsetting_diagram.png?raw=true){fig-alt=\"Three maps of the United States are present, with a red bounding box over the state of Colorado. Filtering and subsetting are demonstrated by overlaying SMAP L2 data, with data overlapping and cropping the rectangle, respectively.\"  width=60%}\n",
    "\n",
    "There are three primary types of subsetting that we will walk through: \n",
    "1. Temporal\n",
    "2. Spatial\n",
    "3. Variable\n",
    "\n",
    "In each case, we will be excluding parts of the dataset that are not wanted using `xarray`. Note that \"subsetting\" is also called a data \"transformation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228b00d-9659-4766-b25b-d9ba82506006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.time.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc1e31-656b-4a02-a80f-b6ea11712068",
   "metadata": {},
   "source": [
    "We start with a subset that represents the U.S. state of California.  Notice the dimensions of the Dataset and each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5b217-246c-4549-8e7d-773dec50bf40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_subset = ds.sel(time='2023-02-24', lat=slice(31, 43), lon=slice(-125, -113)) \n",
    "ds_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210377ad-4dd0-401d-aad8-0d1760941daf",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "We will first plot using the methods built-in to the `xarray` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b5757-eb9e-4816-b2c8-5516823080ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_subset['precipitation'].squeeze().plot(figsize=(10,6), x='lon', y='lat');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d6593-7d61-4a8a-8873-e313b2474b5a",
   "metadata": {},
   "source": [
    "Now let's utilize the \"Probability of liquid precipitation phase\" (`probabilityLiquidPrecipitation`) variable to split apart the snow precipitation from everything else.  And we'll utilize `precipitation_cnt_cond` to filter out data points that had less than 0.01 mm/hr preciptation amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8222e6-df93-496b-ac47-0dcff427fb33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snow = ds_subset['precipitation'].where(\n",
    "    (ds_subset.precipitation_cnt_cond>0) & (ds_subset.probabilityLiquidPrecipitation == 1)\n",
    ")\n",
    "\n",
    "prcp = ds_subset['precipitation'].where(\n",
    "    (ds_subset.precipitation_cnt_cond>0) & (ds_subset.probabilityLiquidPrecipitation != 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0834c9e-653e-4d3d-9114-bf927d6c984b",
   "metadata": {},
   "source": [
    "In the following plotting commands, we utilize `cartopy` and `matplotlib` to generate a more customized figure. \n",
    "\n",
    "`cartopy` is used to set the map projection (to PlateCarree) and to add U.S. state boundary lines to the figure. `matplotlib`'s pcolormesh is used to generate the color plot, with colors determined by the third argument's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9dcc1-c311-4bef-8a65-08beb54f0453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the plot\n",
    "proj = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(8,5), dpi=130, facecolor=\"w\", subplot_kw=dict(projection=proj))\n",
    "\n",
    "snowax = plt.pcolormesh(prcp.lon, prcp.lat, snow.squeeze(), vmax=53, cmap='cool')\n",
    "prcpax = plt.pcolormesh(prcp.lon, prcp.lat, prcp.squeeze(), vmax=53, cmap='RdYlGn')\n",
    "\n",
    "plt.colorbar(snowax, ax=ax, label=\"snow (mm/day)\")\n",
    "plt.colorbar(prcpax, ax=ax, label=\"rainfall (mm/day)\")\n",
    "ax.add_feature(cfeature.STATES)\n",
    "ax.set_extent([-125, -113.0, 31.0, 43.0], crs=proj)\n",
    "ax.set_title(f'Precipitation {date_start}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dcf1c8-e95f-4a3a-8c8a-bed8a9609fb2",
   "metadata": {},
   "source": [
    "## Example 2 - Xarray Subsetting - Snow Cover from MODIS/Terra, Daily Level 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1ea28e-fcae-45bb-a917-e047fcae5bf5",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will use MODIS/Terra Snow Cover Daily L3 Global 0.05Deg CMG. The Moderate Resolution Imaging Spectroradiometer (MODIS) global Level-3 (L3) data set provides the percentage of snow-covered land and cloud-covered land observed daily, within 0.05° (approx. 5 km) MODIS Climate Modeling Grid (CMG) cells. \n",
    "\n",
    "The dataset and algorithm is described in the [data user guide](https://nsidc.org/sites/default/files/mod10c1-v061-userguide_0.pdf) and the [Product Specific Document](https://nsidc.org/sites/default/files/c61_modis_snow_user_guide.pdf). \n",
    "\n",
    "Please cite the dataset as:\n",
    "> Hall, D. K. and G. A. Riggs. (2021). MODIS/Terra Snow Cover Daily L3 Global 0.05Deg CMG, Version 61. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/MODIS/MOD10C1.061."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1ef86-595f-407c-a8e1-1601ccb79e66",
   "metadata": {},
   "source": [
    "Using the dataset DOI, we will use the earthaccess module to search for dataset granules from February 24, 2023, and March 2, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a0f5a-df1e-46e6-9359-6c66c2076ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doi = '10.5067/MODIS/MOD10C1.061' # MODIS Terra Snowcover\n",
    "\n",
    "# search granules from Feb 15, 2023\n",
    "date1 = \"2023-02-15\"\n",
    "granules1 = earthaccess.search_data(\n",
    "    count=-1, # needed to retrieve all granules\n",
    "    doi=doi,\n",
    "    temporal=(date1, date1)\n",
    ")\n",
    "# search granules from March 02, 2023\n",
    "date2 = \"2023-03-02\"\n",
    "granules2 = earthaccess.search_data(\n",
    "    count=-1, # needed to retrieve all granules\n",
    "    doi=doi,\n",
    "    temporal=(date2, date2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8673d7-6722-4a94-a580-209b972c84de",
   "metadata": {},
   "source": [
    "Let's download the granules to the local environment. This is needed as direct access to HDF4 files that MODIS Collection 6.1 comes as is currently not supported. The `earthaccess` module manages the authentication that is required for accessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092293b9-0be3-4997-b4ae-1f5bc4d4d8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "earthaccess.download(granules1, local_path='.')\n",
    "earthaccess.download(granules2, local_path='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6f6c9-aae9-41d2-8d9a-0e85faae0553",
   "metadata": {},
   "source": [
    "### Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da1f1f-6979-4081-ad29-6e741adfab3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's open the downloaded granules into a `rioxarray`. The variable `Day_CMG_Snow_Cover` provides daily percent snow in 5km grids. The variable `Snow_Spatial_QA` provides quality indicator for each grid: 0=best, 1=good, 2=ok, 3=poor, 4=other, 237=inland water, 239=ocean, 250=cloud obscured water 252=Antarctica mask, 253=not mapped, 254=no retrieval, and 255=fill. We will only use the grids with 0, 1, and 2 quality flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa35645-b2a6-4699-bd9f-b55d41342844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open granule from Feb 15, 2023\n",
    "g_1 = Path(Path(granules1[0].data_links()[0]).name)\n",
    "if g_1.is_file():\n",
    "    with rxr.open_rasterio(g_1) as modis:\n",
    "        print(modis)\n",
    "        snow_cover1 = modis['Day_CMG_Snow_Cover'][:]\n",
    "        snow_cover_qa1 = modis['Snow_Spatial_QA'][:]\n",
    "\n",
    "# open granules from March 02, 2023\n",
    "g_2 = Path(Path(granules2[0].data_links()[0]).name)\n",
    "if g_2.is_file():\n",
    "    with rxr.open_rasterio(g_2) as modis:\n",
    "        snow_cover2 = modis['Day_CMG_Snow_Cover'][:]\n",
    "        snow_cover_qa2 = modis['Snow_Spatial_QA'][:]\n",
    "\n",
    "# Spatially subset and keep only good quality cells\n",
    "snow_cover_good1 = (\n",
    "    snow_cover1\n",
    "    .sel(x=slice(-125, -113), y=slice(43, 31))\n",
    "    .where((snow_cover_qa1 >= 0) & (snow_cover_qa1 <= 2))\n",
    ")\n",
    "snow_cover_good2 = (\n",
    "    snow_cover2\n",
    "    .sel(x=slice(-125, -113), y=slice(43, 31))\n",
    "    .where((snow_cover_qa2 >= 0) & (snow_cover_qa2 <= 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f4734-4144-400d-929c-b6ed3d5d7ab0",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93ef69-dba9-4e2a-b4e6-2e857404dc66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the plot\n",
    "proj = ccrs.PlateCarree()\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4), dpi=130, facecolor=\"w\", subplot_kw=dict(projection=proj))\n",
    "\n",
    "snowax1 = ax1.pcolormesh(snow_cover_good1.x.values, snow_cover_good1.y.values, snow_cover_good1.values[0], vmax=100, cmap='Blues')\n",
    "plt.colorbar(snowax1, ax=ax1, label=\"snow cover (%)\")\n",
    "ax1.add_feature(cfeature.STATES)\n",
    "ax1.set_title(f'Snow Cover {date1}')\n",
    "\n",
    "snowax2 = ax2.pcolormesh(snow_cover_good2.x.values, snow_cover_good2.y.values, snow_cover_good2.values[0], vmax=100, cmap='Blues')\n",
    "plt.colorbar(snowax2, ax=ax2, label=\"snow cover (%)\")\n",
    "ax2.add_feature(cfeature.STATES)\n",
    "ax2.set_title(f'Snow Cover {date2}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc0a27-2fbe-4cca-a3d1-095fa6a2b60c",
   "metadata": {},
   "source": [
    "## Example 3 - Harmony-py Subsetting - Precipitable Water from DSCOVR-EPIC Composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d10aeb-a770-4988-bd1e-0d8b02f818c5",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The NASA Earth Polychromatic Imaging Camera (EPIC)-view Multi-Sensor Global Cloud and Radiance Composites are generated by optimally merging together multiple imagers on low Earth orbit (LEO) satellites (including MODIS, VIIRS, and AVHRR) and geostationary (GEO) satellites (including GOES-13 and -15, METEOSAT-7 and -10, MTSAT-2, and Himawari-8). These provide a seamless global composite product at 5-km resolution by using an aggregated rating that considers five parameters (nominal satellite resolution, pixel time relative to the Earth Polychromatic Imaging Camera (EPIC) observation time, viewing zenith angle, distance from day/night terminator, and sun glint factor) and selects the best observation at the time nearest to the EPIC measurements. The global composite data are then remapped into the EPIC Field of View (FOV) by convolving the high-resolution cloud properties with the EPIC point spread function (PSF) defined with a half-pixel accuracy to produce the EPIC composite. PSF-weighted radiances and cloud properties averages are computed separately for each cloud phase. Ancillary data (i.e., surface type, snow and ice map, skin temperature, precipitable water, etc.) needed for anisotropic factor selections are also included in the composite. These composite images are produced for each observation time of the EPIC instrument (typically 300 to 600 composites per month).\n",
    "\n",
    "\n",
    "The dataset and development of the composite product is described in the [Khlopenkov et al., 2017](https://doi.org/10.1117/12.2278645) and the [Product Description page](http://doi.org/10.5067/EPIC/DSCOVR/L2_COMPOSITE_01). This dataset can also be viewed in [Earthdata Search](https://cmr.earthdata.nasa.gov/search/concepts/C1576365803-LARC_ASDC.html). \n",
    "\n",
    "Please cite the dataset as:\n",
    "> NASA/LARC/SD/ASDC. (2017). EPIC-view satellite composites for DSCOVR, Version 1 [Data set]. NASA Langley Atmospheric Science Data Center DAAC. Retrieved from https://doi.org/10.5067/EPIC/DSCOVR/L2_COMPOSITE_01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0df8ec4-6c65-4746-8c06-c6b8e42eaadb",
   "metadata": {},
   "source": [
    "### Harmony\n",
    "\n",
    "[Harmony](https://www.earthdata.nasa.gov/learn/articles/harmony-in-the-cloud) is the behind-the-scenes orchestrator for much of the cloud-based transformations happening on NASA's [Earthdata Search](https://search.earthdata.nasa.gov/search) interface. However, requests can also be sent directly to Harmony in a programmatic fashion, either through use of the `harmony-py` Python library or through transmitting underlying HTTP requests.  In this example, we demonstrate the use of `harmony-py`, which was created as an alternative to Harmony's RESTful Application Programming Interface (API) and to make it more convenient to invoke Harmony directly from a Python environment, such as Jupyter notebooks or larger Python applications.\n",
    "\n",
    "Note that additional examples can be found on the `harmony-py` GitHub page [here](https://github.com/nasa/harmony-py/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e324b-88c9-4726-af33-ff07367dcf28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "harmony_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb3263-7a60-4e17-b2d0-2f9189badceb",
   "metadata": {},
   "source": [
    "#### Inspecting a data collection for its capabilities and variables\n",
    "\n",
    "The harmony-py package provides useful functions for requesting a report of the capabilities that are configured for a given collection. We will use that function here to inspect the DSCOVR EPIC-view Composite collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db162b89-de86-4bb0-b620-4257d9af6b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_id = \"C1576365803-LARC_ASDC\"  # EPIC-view satellite composites for DSCOVR, Version 1\n",
    "# collection_id = \"C1996881146-POCLOUD\"\n",
    "\n",
    "capabilities_request = CapabilitiesRequest(collection_id=collection_id)\n",
    "\n",
    "capabilities = harmony_client.submit(capabilities_request)\n",
    "capabilities_str = json.dumps(capabilities, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90865d24-6bbc-46ff-97a4-05f7d8fb4f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(json.dumps({key: val for key, \n",
    "                  val in capabilities.items() \n",
    "                  if key not in ['services', 'variables']})\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2a5b3-ff8f-4624-9232-1a73436ba557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint([v[\"name\"] for v in capabilities[\"variables\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcca980-e6c2-4252-9281-d52e0d8f60e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(capabilities[\"services\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f65119-5436-4701-ac48-bf64d6d163cc",
   "metadata": {},
   "source": [
    "### Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8f6a3-5278-4ca7-87e5-75f10deec65c",
   "metadata": {},
   "source": [
    "#### Define an area of interest\n",
    "\n",
    "For this example, we will use a GeoJSON to specify a non-rectangular region instead of a simpler, rectangular bounding box.  We will use the GeoJSON that defines a region around San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33249e74-603d-4030-a2df-1b19df50fce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the GeoJSON into a GeoDataFrame\n",
    "sf_geojson = '../../2023-Cloud-Workshop-AGU/data/sf_to_sierranvmt.geojson'\n",
    "gdf = gpd.read_file(sf_geojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c13872-3ca2-436e-ae53-3c22ce07e60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define a Geoview Point so we can visualize the area of interest in relation to San Francisco\n",
    "sf = (-122.42, 37.77, 'SanFrancisco')\n",
    "cities_lonlat = gv.Points([sf], vdims='City')\n",
    "\n",
    "# Generate an image\n",
    "base = gv.tile_sources.EsriImagery.opts(width=650, height=500)\n",
    "ocean_map = gv.Polygons(gdf).opts(line_color='yellow', line_width=5, color=None)\n",
    "base * ocean_map * cities_lonlat.options(size=20, color='red', marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e01ba7d-e499-4869-a835-6e2349303e21",
   "metadata": {},
   "source": [
    "#### Build a Harmony subsetting request\n",
    "\n",
    "A Harmony request can include spatial, temporal, and variable subsetting all in the same request.  Here we will request all three types of subsetting to be performed on the EPIC-view Composite dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d190d-81f3-4fd2-b92e-5345769b85ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_id = \"C1576365803-LARC_ASDC\"  # (~9 to 12 minutes to process) EPIC-view satellite composites for DSCOVR, Version 1\n",
    "# collection_id = \"C1996881146-POCLOUD\"  # (?)\n",
    "\n",
    "request = Request(\n",
    "    collection=Collection(id=collection_id),\n",
    "    shape=sf_geojson,\n",
    "    temporal={\n",
    "        'start': dt.datetime(2016, 2, 24, 0),\n",
    "        'stop': dt.datetime(2016, 2, 24, 23)   \n",
    "    },\n",
    "    variables=[\"lat\", \"lon\", \"time\", \"mask\", \"analysed_sst\", \"sst_anomaly\"]\n",
    "    # variables=[\"general/relative_time\", \"general/precipitable_water\", \"clear_sky/skin_temperature\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca89302-42f0-4017-ba0f-a6e348b8dc1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_id = harmony_client.submit(request)\n",
    "job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b95c6-4312-4d99-acfc-6eea0e28056d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "harmony_client.status(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe3ac3-5172-4218-ba44-0b063abdbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony_client.wait_for_processing(job_id, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf99d8-74c1-49cf-b65d-38ea1ef0e5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "harmony_client.status(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d996a53-6cf4-40c3-a5c5-06d932956609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = harmony_client.result_json(job_id, show_progress=True)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f432ab-17aa-4fe8-b99a-831dd59725ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\nDownloading results:')\n",
    "futures = harmony_client.download_all(job_id)\n",
    "\n",
    "filenames = []\n",
    "for f in futures:\n",
    "    fn = f.result()\n",
    "    filenames.append(fn)\n",
    "    print(fn)  # f.result() is a filename, in this case\n",
    "\n",
    "print('\\nDone downloading.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b5dea-e0ba-4574-90b7-f087c94605b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_ds_general = xr.open_dataset(filenames[0], group=\"general\", decode_times=False)\n",
    "new_ds_general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc0c90-8a6d-4248-ab94-4b8e6afdedc2",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01834c-1ce1-4993-8b43-9a8bc9a862b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flatten and remove null data points in preparation for plotting.\n",
    "lons = new_ds_general['lon'].values.flatten()\n",
    "lats = new_ds_general['lat'].values.flatten()\n",
    "vals = new_ds_general['precipitable_water'].values.flatten()\n",
    "good_idcs = ~np.isnan(lons) & ~np.isnan(lats) & ~np.isnan(vals)\n",
    "lons = lons[good_idcs]\n",
    "lats = lats[good_idcs]\n",
    "vals = vals[good_idcs]\n",
    "\n",
    "\n",
    "# Create figure\n",
    "proj = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=(6, 4), facecolor=\"w\", subplot_kw=dict(projection=proj))\n",
    "\n",
    "ax_handle = ax.scatter(lons, lats, c=vals, cmap=\"cool\")\n",
    "plt.colorbar(ax_handle, ax=ax, label=\"precipitable water\")\n",
    "\n",
    "ax.add_feature(cfeature.STATES)\n",
    "ax.set_extent([-125, -113.0, 31.0, 43.0], crs=proj)\n",
    "# ax1.set_title(f'{date}')\n",
    "\n",
    "ax.set_xticks([-125, -120, -115, -110], crs=proj)\n",
    "ax.set_yticks([32, 34, 36, 38, 40, 42], crs=proj)\n",
    "lon_formatter = LongitudeFormatter(number_format='.1f',\n",
    "                                   degree_symbol='',\n",
    "                                   dateline_direction_label=True)\n",
    "lat_formatter = LatitudeFormatter(number_format='.1f',\n",
    "                                  degree_symbol='')\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "ax.yaxis.set_major_formatter(lat_formatter)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f811e-19fd-4269-b9e9-4b97baef1f70",
   "metadata": {},
   "source": [
    "## Appendix - Snow Mass from SMAP, 3-hourly Level 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348bfa6-19e2-463e-a60d-6f1287e7b831",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The Soil Moisture Active Passive (SMAP) L4 Global 3-hourly 9 km EASE-Grid Surface and Root Zone Soil Moisture Geophysical Data (SPL4SMGP) provides a model-derived global 3-hr time average of snow mass in kg/m2.  SMAP L-band brightness temperature data from descending and ascending half-orbit satellite passes (approximately 6:00 a.m. and 6:00 p.m. local solar time, respectively) are assimilated into a land surface model that is gridded using an Earth-fixed, global cylindrical 9 km Equal-Area Scalable Earth Grid, Version 2.0 (EASE-Grid 2.0) projection. Snow mass estimates are based on a snow model component of the NASA Catchment Land Surface Model.\n",
    "\n",
    "The dataset and algorithm are described in the [data user guide](https://nsidc.org/sites/default/files/documents/user-guide/multi_spl4smau-v007-userguide.pdf) and the [Product Specific Document](https://nsidc.org/sites/default/files/documents/technical-reference/reichle1438.pdf). \n",
    "\n",
    "Please cite the dataset as:\n",
    "> Reichle, R., G. De Lannoy, R. D. Koster, W. T. Crow, J. S. Kimball, Q. Liu, and M. Bechtold. (2022). SMAP L4 Global 3-hourly 9 km EASE-Grid Surface and Root Zone Soil Moisture Geophysical Data, Version 7. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/EVKPQZ4AFC4D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad33c72-b484-4687-bc6c-0822804422de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SMAP SPL4SMGP\n",
    "doi = '10.5067/EVKPQZ4AFC4D'\n",
    "\n",
    "# search granules from Feb 15, 2023\n",
    "date1 = \"2023-02-15\"\n",
    "granules1 = earthaccess.search_data(\n",
    "    count=-1, # needed to retrieve all granules\n",
    "    doi=doi,\n",
    "    temporal=(date1, date1)\n",
    ")\n",
    "\n",
    "# search granules from March 02, 2023\n",
    "date2 = \"2023-03-02\"\n",
    "granules2 = earthaccess.search_data(\n",
    "    count=-1, # needed to retrieve all granules\n",
    "    doi=doi,\n",
    "    temporal=(date2, date2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96e00d-d1cf-40cd-bd9e-3e620929a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# granules from Feb 15, 2023\n",
    "fh1 = earthaccess.open(granules1)\n",
    "# open geophysical_data group\n",
    "ds1 = xr.open_dataset(fh1[0], phony_dims='access', group='Geophysical_Data')\n",
    "# get location\n",
    "ds_loc1 = xr.open_dataset(fh1[0], phony_dims='access')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3137cff-879c-41dd-bb68-fdf107f00d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# granules from March 02, 2023\n",
    "fh2 = earthaccess.open(granules2)\n",
    "# open geophysical_data group\n",
    "ds2 = xr.open_dataset(fh2[0], phony_dims='access', group='Geophysical_Data')\n",
    "# get location\n",
    "ds_loc2 = xr.open_dataset(fh2[0], phony_dims='access')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557bace4-0e08-49ce-8b9a-a72b2fecbada",
   "metadata": {},
   "source": [
    "The \"snow_mass\" variable is within the \"Geophysical_Data\" group. It provides the average snow mass (or snow water equivalent) over a land fraction of the grid cell, excluding areas of open water and permanent ice. Let's print the attributes of the `snow_mass` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc16f5-9aad-4b51-9d92-c547fbda64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.variables['snow_mass'].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226c00a-ba07-4481-8f24-cac285a26ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the plot\n",
    "proj = ccrs.Projection(\"EPSG:6933\") # EASEGRID 2\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4), dpi=130, facecolor=\"w\", subplot_kw=dict(projection=proj))\n",
    "\n",
    "ca_bounds = [-12060785, -10902950, 3769089, 4995383]\n",
    "\n",
    "snow_mass1 = ds1.snow_mass.where(ds1.snow_mass>9.4)\n",
    "snowax1 = ax1.pcolormesh(ds_loc1.x, ds_loc1.y, snow_mass1, vmax=200, cmap='Blues')\n",
    "plt.colorbar(snowax1, ax=ax1, label=\"snow mass (kg/m2)\")\n",
    "ax1.add_feature(cfeature.STATES)\n",
    "ax1.set_extent(ca_bounds, crs=proj)\n",
    "ax1.set_title(f'Snow Mass {date1}')\n",
    "\n",
    "snow_mass2 = ds2.snow_mass.where(ds2.snow_mass>9.4)\n",
    "snowax2 = ax2.pcolormesh(ds_loc2.x, ds_loc2.y, snow_mass2, vmax=200, cmap='Blues')\n",
    "plt.colorbar(snowax2, ax=ax2, label=\"snow mass (kg/m2)\")\n",
    "ax2.add_feature(cfeature.STATES)\n",
    "ax2.set_extent(ca_bounds, crs=proj)\n",
    "ax2.set_title(f'Snow Mass {date2}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92fdcc-8868-4289-812d-3df354aba8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e6b9c23-5ec8-43e0-83fe-bcc7189356b6",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Tutorials\n",
    "\n",
    "This clinic was based off of several notebook tutorials including those presented during [past workshop events](https://nasa-openscapes.github.io/earthdata-cloud-cookbook/tutorials/), along with other materials co-created by the NASA Openscapes mentors:\n",
    "* [2021 Earthdata Cloud Hackathon](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/)\n",
    "* [2021 AGU Workshop](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/)\n",
    "* [Accessing and working with ICESat-2 data in the cloud](https://github.com/nsidc/NSIDC-Data-Tutorials/tree/main/notebooks/ICESat-2_Cloud_Access)\n",
    "* [Analyzing Sea Level Rise Using Earth Data in the Cloud](https://github.com/betolink/earthaccess-gallery/blob/main/notebooks/Sea_Level_Rise/SSL.ipynb)\n",
    "\n",
    "### Cloud services\n",
    "\n",
    "The examples used in the clinic provide an abbreviated and simplified workflow to explore access and subsetting options available through the Earthdata Cloud. There are several other options that can be used to interact with data in the Earthdata Cloud including: \n",
    "\n",
    "* [OPeNDAP](https://opendap.earthdata.nasa.gov/) \n",
    "    * Hyrax provides direct access to subsetting of NASA data using Python or your favorite analysis tool\n",
    "    * Tutorial highlighting OPeNDAP usage: https://nasa-openscapes.github.io/earthdata-cloud-cookbook/how-tos/working-locally/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html\n",
    "* [Zarr-EOSDIS-Store](https://github.com/nasa/zarr-eosdis-store)\n",
    "    * The zarr-eosdis-store library allows NASA EOSDIS Collections to be accessed efficiently by the Zarr Python library, provided they have a sidecar DMR++ metadata file generated. \n",
    "    * Tutorial highlighting this library's usage: https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/09_Zarr_Access.html \n",
    "\n",
    "### Support\n",
    "\n",
    "* [Earthdata Forum](https://forum.earthdata.nasa.gov/)\n",
    "    * User Services and community support for all things NASA Earthdata, including Earthdata Cloud\n",
    "* [Earthdata Webinar series](https://www.earthdata.nasa.gov/learn/webinars-and-tutorials)\n",
    "    * Webinars from DAACs and other groups across EOSDIS including guidance on working with Earthdata Cloud\n",
    "    * See the [Earthdata YouTube channel](https://www.youtube.com/@NASAEarthdata/featured) for more videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3fd72-b963-4ad3-a4b2-5124fdd60360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
