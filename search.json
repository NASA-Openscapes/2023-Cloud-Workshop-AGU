[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "section": "",
    "text": "Welcome to the 2023 NASA Openscapes AGU Workshop hosted by NASA Openscapes!\nThe workshop will take place in person and virtually at AGU on Sunday, December 10, 2023 from 1pm-4:30pm PST (UTC-7)."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "section": "",
    "text": "Welcome to the 2023 NASA Openscapes AGU Workshop hosted by NASA Openscapes!\nThe workshop will take place in person and virtually at AGU on Sunday, December 10, 2023 from 1pm-4:30pm PST (UTC-7)."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "section": "About",
    "text": "About\n\nWorkshop Goals\n\nProvide an inclusive place to learn about and experience working with NASA Earthdata Cloud;\nProvide strategies and best practices for finding and accessing NASA Earthdata Cloud data to help promote interdisciplinary research. Specifically, participants will learn how to access data from AWS S3 buckets and have a better understanding of the Python ecosystem used to analyze the data depending on size and data formats;\nBetter understand the needs of Earthdata data scientists by providing a pre/post survey and engaging in discussions with workshop participants;\nIdentify and practice using popular open source tools and utilities.\n\n\n\nWorkshop Description\nEarth science data, including NASA Earthdata, is increasingly more available from the cloud. By leveraging cloud compute resources, open science principles, and technologies that foster transparency and reproducibility, science can be done at rates and scales that are not achievable by using personal/local machines. Building modern data skills in a friendly environment is crucial for accelerating science and reducing barriers to inclusive scientific research. In this workshop, NASA Openscapes Mentors from NASA’s Earth Observing System Data and Information System (EOSDIS) DAACs (data centers) will teach the foundations of an open science mindset and apply these concepts to work in the cloud with NASA Earthdata. Participants will take part in hands-on tutorials using a JupyterHub managed by 2i2c in AWS. Participants will leave with a better understanding of how to leverage data and services from NASA Earthdata Cloud within their work across a variety of disciplines and data types, as well as how to apply the concepts of open science as a daily practice. The workshop will encourage discussion and reflection on how Earth science is evolving. Tutorials will be taught in Python. The target audience is anyone interested in using NASA Earth Science data within the AWS cloud. Previous experience in the AWS cloud is not necessary. Experience using Python is recommended but not required.\n\n\nLearning Outcomes\nUpon completion of the proposed workshop, participants will leave with a better understanding of how NASA Earthdata Cloud data and services can best be leveraged and integrated within their work across a variety of disciplines and data types, while being exposed to open science practices and workflows in Python. Participants will be engaged on two different levels: (1) through an open science mindset, and (2) through science use cases that demonstrate capabilities for efficient data discovery, access, and use within the cloud. These learning objectives will encourage discussion and reflection on how the Earth science, data science, and informatics communities are evolving, and how this evolution presents challenges and opportunities in scientific research."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis Workshop has been developed as a cross-DAAC collaboration by the NASA-Openscapes Team. Learn more at nasa-openscapes.github.io."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Workshop Slides",
    "section": "",
    "text": "TODO: embed slides"
  },
  {
    "objectID": "slides.html#enabling-analysis-in-the-cloud-using-nasa-earth-science-data",
    "href": "slides.html#enabling-analysis-in-the-cloud-using-nasa-earth-science-data",
    "title": "Workshop Slides",
    "section": "",
    "text": "TODO: embed slides"
  },
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up. Please remember your username and password!\n\n\n\nA GitHub account is required to gain access to the provided 2i2c cloud computing platform. Please visit https://github.com/join to register and create a free GitHub account. Make sure you have sent your username to the workshop leaders once you have done so for access to our cloud computing environment.\n\n\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2."
  },
  {
    "objectID": "prerequisites.html#before-the-workshop-you-will-need-the-following",
    "href": "prerequisites.html#before-the-workshop-you-will-need-the-following",
    "title": "Prerequisites",
    "section": "",
    "text": "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up. Please remember your username and password!\n\n\n\nA GitHub account is required to gain access to the provided 2i2c cloud computing platform. Please visit https://github.com/join to register and create a free GitHub account. Make sure you have sent your username to the workshop leaders once you have done so for access to our cloud computing environment.\n\n\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2."
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html",
    "title": "NASA Earthdata Cloud Clinic",
    "section": "",
    "text": "Welcome to the NASA Earthdata Cloud Clinic.\nWe will go through two different direct cloud access & subsetting options available in the Earthdata Cloud:\n\nThe earthaccess python library for data search and direct cloud access, followed by xarray subsetting\n\nWorking with MEaSUREs Sea Surface Height Anomalies\nDiscover data using Earthdata Search\n\nThe Harmony-py python library for direct cloud access & data subsetting\n\nRequesting a subset of data from the GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (v4.1) dataset using a vector-based geospatial file.\nThis dataset can also be viewed in Earthdata Search.\n\n\nIn both scenarios, we will be accessing data directly from Amazon Web Services (AWS), specifically in the us-west-2 region, which is where all cloud-hosted NASA Earthdata reside. This shared compute environment (JupyterHub) is also running in the same location. We will then load the data into Python as an xarray dataset.\nSee the bottom of the notebook for additional resources, including several tutorials that that served as a foundation for this clinic.\n\n\nIn addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection/regridding, and file aggregation. What does subsetting mean? Here’s a generalized graphic of what we mean.\n\nNote: “direct cloud access” is also called “direct S3 access” or simply “direct access”. And “subsetting” is also called “transformation”."
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#summary",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#summary",
    "title": "NASA Earthdata Cloud Clinic",
    "section": "",
    "text": "Welcome to the NASA Earthdata Cloud Clinic.\nWe will go through two different direct cloud access & subsetting options available in the Earthdata Cloud:\n\nThe earthaccess python library for data search and direct cloud access, followed by xarray subsetting\n\nWorking with MEaSUREs Sea Surface Height Anomalies\nDiscover data using Earthdata Search\n\nThe Harmony-py python library for direct cloud access & data subsetting\n\nRequesting a subset of data from the GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (v4.1) dataset using a vector-based geospatial file.\nThis dataset can also be viewed in Earthdata Search.\n\n\nIn both scenarios, we will be accessing data directly from Amazon Web Services (AWS), specifically in the us-west-2 region, which is where all cloud-hosted NASA Earthdata reside. This shared compute environment (JupyterHub) is also running in the same location. We will then load the data into Python as an xarray dataset.\nSee the bottom of the notebook for additional resources, including several tutorials that that served as a foundation for this clinic.\n\n\nIn addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection/regridding, and file aggregation. What does subsetting mean? Here’s a generalized graphic of what we mean.\n\nNote: “direct cloud access” is also called “direct S3 access” or simply “direct access”. And “subsetting” is also called “transformation”."
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#learning-objectives",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#learning-objectives",
    "title": "NASA Earthdata Cloud Clinic",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUtilize the earthaccess python library to search for data using spatial and temporal filters and explore search results\nPerform in-region direct access of data from an Amazon Simple Storage Service (S3) bucket\nExtract variables and spatial slices from an xarray dataset\nPlot data using xarray\nConceptualize data subsetting services provided by NASA Earthdata, including Harmony\nPlot a polygon geojson file with a basemap using geoviews\nUtilize the harmony-py library to request data over the Gulf of Mexico"
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#prerequisites",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#prerequisites",
    "title": "NASA Earthdata Cloud Clinic",
    "section": "Prerequisites",
    "text": "Prerequisites\nFirst we’ll import python packages and set our authentication that will be used for both of our access and subsetting methods.\nYou’ll also need to be aware that data in NASA’s Earthdata Cloud reside in Amazon Web Services (AWS) Simple Storage Service (S3) buckets. Access is provided via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region. While this compute location is required for direct S3 access, all data in Earthdata Cloud are still freely available via download.\n\nImport Required Packages\n\n# Suppress warnings\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\n\n# Direct access\nimport earthaccess \nfrom pprint import pprint\nimport xarray as xr\n\n# Harmony\nimport geopandas as gpd\nimport geoviews as gv\ngv.extension('bokeh', 'matplotlib')\nfrom harmony import BBox, Client, Collection, Request, LinkType\nimport datetime as dt\nimport s3fs\n%matplotlib inline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n  \n  \n\n\n\n\n\n\n\nAbout earthaccess\nearthaccess leverages the Common Metadata Repository (CMR) API to search for collections and granules.\n\nAuthentication\nSearch\n\nTransition from search results to xarray\n\n\nAuthentication for NASA Earthdata\nAn Earthdata Login account is required to access data from NASA Earthdata. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\nThe first step is to get the correct authentication that will allow us to get cloud-hosted data from NASA. This is all done through Earthdata Login. We can use the earthaccess library here, where the login method also gets the correct AWS credentials.\n\nauth = earthaccess.login()\n# are we authenticated?\nif not auth.authenticated:\n    # ask for credentials and persist them in a .netrc file\n    auth.login(strategy=\"interactive\", persist=True)\n\nWe are already authenticated with NASA EDL\n\n\n\n\nSearch for data\nEarthdata Search also uses the CMR API. Let’s head back to our Earthdata Search results to gather more information about our dataset of interest. The dataset “short name” can be found by clicking on the Info button on our collection search result, and we can paste that into a python variable.\n\n\n\nShortname\nCollection Concept ID\nDOI\n\n\n\n\nGPM_3IMERGDF\nC2723754864-GES_DISC\n10.5067/GPM/IMERGDF/DAY/07\n\n\nMOD10C1\nC1646609808-NSIDC_ECS\n10.5067/MODIS/MOD10C1.061\n\n\nSPL4SMGP\nC2531308461-NSIDC_ECS\n10.5067/EVKPQZ4AFC4D\n\n\nSPL4SMAU\nC2537927247-NSIDC_ECS\n10.5067/LWJ6TF5SZRG3\n\n\n\n\nSearch by collection\n\ncollection_id = 'C2723754864-GES_DISC'\n\n\nresults = earthaccess.search_data(\n    concept_id = collection_id,\n    cloud_hosted = True,\n    count = 10    # Restricting to 10 records returned\n)\n\nGranules found: 8400\n\n\nIn this example we used the concept_id parameter to search from our desired collection. However, there are multiple ways to specify the collection(s) we are interested in. Alternative parameters include:\n\ndoi - request collection by digital object indentifier (e.g., doi = ‘10.5067/GPM/IMERGDF/DAY/07’)\n\nshort_name - request collection by CMR shortname (e.g., short_name = ‘GPM_3IMERGDF’)\n\nWe can refine our search by passing more parameters that describe the spatiotemporal domain of our use case. Here, we use the temporal parameter to request a date range and the bounding_box parameter to request granules that intersect with a bounding box.\nFor our bounding box, we are going to read in a GeoJSON file containing a single feature and extract the coordinate pairs for the southeast corner and the northwest corner (or lowerleft and upperright corners) of the bounding box around the feature.\n\ninGeojson = gpd.read_file('../../2023-Cloud-Workshop-AGU/data/sf_to_sierranvmt.geojson')\n\n\nxmin, ymin, xmax, ymax = inGeojson.total_bounds\n\nWe will assign our start date and end date to a varialbe named date_range and we’ll assign the southeast and the northwest corner coordinates to a variable named bbox to be passed to our earthaccess search request.\n\ndate_range = (\"2022-11-19\", \"2023-04-06\")\nbbox = (-127.0761, 31.6444, -113.9039, 42.6310)\n#bbox = (xmin, ymin, xmax, ymax)\n\n\nresults = earthaccess.search_data(\n    concept_id = collection_id,\n    #cloud_hosted = True,\n    temporal = date_range,\n    bounding_box = bbox,\n)\n\nGranules found: 139\n\n\n\nThe short_name and concept_id search parameters can be used to request one or multiple collections per request, but the doi parameter can only request a single collection.\n&gt; concept_ids = [‘C2723754864-GES_DISC’, ‘C1646609808-NSIDC_ECS’]\n\nCMR concept IDs and collection DOIs are unique to each version of a data collection. However CMR shortnames are not. CMR shortnames can be associated with multiple versions of a collection, so it is recommended to use the short_name parameter and the version parameter in conjuction.\n\nUse the cloud_hosted search parameter only to search for data assets available from NASA’s Earthdata Cloud.\n\n\n# col_ids = ['C2723754864-GES_DISC', 'C1646609808-NSIDC_ECS', 'C2531308461-NSIDC_ECS', 'C2537927247-NSIDC_ECS'] \n\n\n# results = earthaccess.search_data(\n#     concept_id = col_ids,\n#     #cloud_hosted = True,\n#     temporal = date_range,\n#     bounding_box = bbox,\n# )"
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#working-with-earthaccess-returns",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#working-with-earthaccess-returns",
    "title": "NASA Earthdata Cloud Clinic",
    "section": "Working with earthaccess returns",
    "text": "Working with earthaccess returns\nearthaccess provides several convienence methods to help streamline processes that historically have be painful when done using traditional methods. Following the search for data, you’ll likely take one of two pathways with those results. You may choose to download the assets that have been returned to you or you may choose to continue working with the search results within the Python environment.\n\nDownload earthaccess results\nearthaccess makes downloading the data from the search results very easy using the earthaccess.download() function.\n\ndownloaded_files = earthaccess.download(\n    results[0:9],\n    local_path='../../2023-Cloud-Workshop-AGU/data',\n)\n\n Getting 9 granules, approx download size: 0.25 GB\nFile 3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4 already downloaded\nFile 3B-DAY.MS.MRG.3IMERG.20221122-S000000-E235959.V07.nc4 already downloaded\nFile 3B-DAY.MS.MRG.3IMERG.20221121-S000000-E235959.V07.nc4 already downloaded\nFile 3B-DAY.MS.MRG.3IMERG.20221120-S000000-E235959.V07.nc4 already downloaded\nFile 3B-DAY.MS.MRG.3IMERG.20221123-S000000-E235959.V07.nc4 already downloaded\nFile 3B-DAY.MS.MRG.3IMERG.20221124-S000000-E235959.V07.nc4 already downloaded\nFile 3B-DAY.MS.MRG.3IMERG.20221125-S000000-E235959.V07.nc4 already downloaded\nFile 3B-DAY.MS.MRG.3IMERG.20221126-S000000-E235959.V07.nc4 already downloaded\nFile 3B-DAY.MS.MRG.3IMERG.20221127-S000000-E235959.V07.nc4 already downloaded\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore earthaccess search response\n\nprint(f'The results variable is a {type(results)} of {type(results[0])}')\n\nThe results variable is a &lt;class 'list'&gt; of &lt;class 'earthaccess.results.DataGranule'&gt;\n\n\n\nlen(results)\n\n139\n\n\n\nitem = results[0]\ntype(item)\n\nearthaccess.results.DataGranule\n\n\n\nitem.keys()\n\ndict_keys(['meta', 'umm', 'size'])\n\n\n\nitem['meta']\n\n{'concept-type': 'granule',\n 'concept-id': 'G2756693035-GES_DISC',\n 'revision-id': 1,\n 'native-id': 'GPM_3IMERGDF.07:3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4',\n 'provider-id': 'GES_DISC',\n 'format': 'application/echo10+xml',\n 'revision-date': '2023-08-25T14:38:26.911Z'}\n\n\n\nGet data URLs / S3 URIs\nGet links to data. The data_links() method is used to return the URL(s)/data link(s) for the item. By defalt the method returns the HTTPS URL to download or access the item.\n\nitem.data_links()\n\n['https://data.gesdisc.earthdata.nasa.gov/data/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4']\n\n\nThe data_links() method can also be used to get the s3 URI when we want to perform direct s3 access of the data in the cloud. To get the s3 URI, pass access = 'direct' to the method.\n\nitem.data_links(access='direct')\n\n['s3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4']\n\n\nFinally, we can extract all of the data links from our search results and add them to a list\n\ndata_link_list = []\n\nfor granule in results:\n    for asset in granule.data_links(access='direct'):\n        data_link_list.append(asset)\n        \n\n\ndata_link_list[0:9]\n\n['s3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221120-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221121-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221122-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221123-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221124-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221125-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221126-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221127-S000000-E235959.V07.nc4']\n\n\n\n\nPass results to xarray\nWe pass the earthaccess.open() function to\n\nfileset = earthaccess.open(results)\n\n Opening 139 granules, approx size: 3.75 GB\nusing endpoint: https://data.gesdisc.earthdata.nasa.gov/s3credentials\n\n\n\n\n\n\n\n\n\n\n\n\nds = xr.open_mfdataset(fileset, chunks = {})\n#ds\n\nSome really cool things just happened here! Not only were we able to seamlessly stream our earthaccess search results into a xarray dataset using the open_mfdataset() (multi-file) method, but earthaccess determined that we were working from within AWS us-west-2 and accessed the data via direct S3 access! We didn’t have to create a session or a filesystem to authenicate and connect to the data. earthaccess did this for us using the auth object we created at the beginning of this tutorial. If we were not working in AWS us-west-2, earthaccess would automagically switch to accessing the data via the HTTPS endpoints and would again handle the authentication for us.\nLet’s take a quick lock at our xarray dataset\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                         (time: 139, lon: 3600, lat: 1800, nv: 2)\nCoordinates:\n  * lon                             (lon) float32 -179.9 -179.9 ... 179.9 179.9\n  * lat                             (lat) float64 -89.95 -89.85 ... 89.85 89.95\n  * time                            (time) datetime64[ns] 2022-11-19 ... 2023...\nDimensions without coordinates: nv\nData variables:\n    precipitation                   (time, lon, lat) float32 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    precipitation_cnt               (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    precipitation_cnt_cond          (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    MWprecipitation                 (time, lon, lat) float32 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    MWprecipitation_cnt             (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    MWprecipitation_cnt_cond        (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    randomError                     (time, lon, lat) float32 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    randomError_cnt                 (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    probabilityLiquidPrecipitation  (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    time_bnds                       (time, nv) datetime64[ns] dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\nAttributes:\n    BeginDate:       2022-11-19\n    BeginTime:       00:00:00.000Z\n    EndDate:         2022-11-19\n    EndTime:         23:59:59.999Z\n    FileHeader:      StartGranuleDateTime=2022-11-19T00:00:00.000Z;\\nStopGran...\n    InputPointer:    3B-HHR.MS.MRG.3IMERG.20221119-S000000-E002959.0000.V07A....\n    title:           GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 ...\n    DOI:             10.5067/GPM/IMERGDF/DAY/07\n    ProductionTime:  2023-08-25T14:03:25.792Zxarray.DatasetDimensions:time: 139lon: 3600lat: 1800nv: 2Coordinates: (3)lon(lon)float32-179.9 -179.9 ... 179.9 179.9units :degrees_eastlong_name :Longitudearray([-179.95, -179.85, -179.75, ...,  179.75,  179.85,  179.95],\n      dtype=float32)lat(lat)float64-89.95 -89.85 ... 89.85 89.95units :degrees_northlong_name :Latitudearray([-89.95, -89.85, -89.75, ...,  89.75,  89.85,  89.95])time(time)datetime64[ns]2022-11-19 ... 2023-04-06standard_name :timelong_name :timebounds :time_bndsarray(['2022-11-19T00:00:00.000000000', '2022-11-20T00:00:00.000000000',\n       '2022-11-21T00:00:00.000000000', '2022-11-22T00:00:00.000000000',\n       '2022-11-23T00:00:00.000000000', '2022-11-24T00:00:00.000000000',\n       '2022-11-25T00:00:00.000000000', '2022-11-26T00:00:00.000000000',\n       '2022-11-27T00:00:00.000000000', '2022-11-28T00:00:00.000000000',\n       '2022-11-29T00:00:00.000000000', '2022-11-30T00:00:00.000000000',\n       '2022-12-01T00:00:00.000000000', '2022-12-02T00:00:00.000000000',\n       '2022-12-03T00:00:00.000000000', '2022-12-04T00:00:00.000000000',\n       '2022-12-05T00:00:00.000000000', '2022-12-06T00:00:00.000000000',\n       '2022-12-07T00:00:00.000000000', '2022-12-08T00:00:00.000000000',\n       '2022-12-09T00:00:00.000000000', '2022-12-10T00:00:00.000000000',\n       '2022-12-11T00:00:00.000000000', '2022-12-12T00:00:00.000000000',\n       '2022-12-13T00:00:00.000000000', '2022-12-14T00:00:00.000000000',\n       '2022-12-15T00:00:00.000000000', '2022-12-16T00:00:00.000000000',\n       '2022-12-17T00:00:00.000000000', '2022-12-18T00:00:00.000000000',\n       '2022-12-19T00:00:00.000000000', '2022-12-20T00:00:00.000000000',\n       '2022-12-21T00:00:00.000000000', '2022-12-22T00:00:00.000000000',\n       '2022-12-23T00:00:00.000000000', '2022-12-24T00:00:00.000000000',\n       '2022-12-25T00:00:00.000000000', '2022-12-26T00:00:00.000000000',\n       '2022-12-27T00:00:00.000000000', '2022-12-28T00:00:00.000000000',\n       '2022-12-29T00:00:00.000000000', '2022-12-30T00:00:00.000000000',\n       '2022-12-31T00:00:00.000000000', '2023-01-01T00:00:00.000000000',\n       '2023-01-02T00:00:00.000000000', '2023-01-03T00:00:00.000000000',\n       '2023-01-04T00:00:00.000000000', '2023-01-05T00:00:00.000000000',\n       '2023-01-06T00:00:00.000000000', '2023-01-07T00:00:00.000000000',\n       '2023-01-08T00:00:00.000000000', '2023-01-09T00:00:00.000000000',\n       '2023-01-10T00:00:00.000000000', '2023-01-11T00:00:00.000000000',\n       '2023-01-12T00:00:00.000000000', '2023-01-13T00:00:00.000000000',\n       '2023-01-14T00:00:00.000000000', '2023-01-15T00:00:00.000000000',\n       '2023-01-16T00:00:00.000000000', '2023-01-17T00:00:00.000000000',\n       '2023-01-18T00:00:00.000000000', '2023-01-19T00:00:00.000000000',\n       '2023-01-20T00:00:00.000000000', '2023-01-21T00:00:00.000000000',\n       '2023-01-22T00:00:00.000000000', '2023-01-23T00:00:00.000000000',\n       '2023-01-24T00:00:00.000000000', '2023-01-25T00:00:00.000000000',\n       '2023-01-26T00:00:00.000000000', '2023-01-27T00:00:00.000000000',\n       '2023-01-28T00:00:00.000000000', '2023-01-29T00:00:00.000000000',\n       '2023-01-30T00:00:00.000000000', '2023-01-31T00:00:00.000000000',\n       '2023-02-01T00:00:00.000000000', '2023-02-02T00:00:00.000000000',\n       '2023-02-03T00:00:00.000000000', '2023-02-04T00:00:00.000000000',\n       '2023-02-05T00:00:00.000000000', '2023-02-06T00:00:00.000000000',\n       '2023-02-07T00:00:00.000000000', '2023-02-08T00:00:00.000000000',\n       '2023-02-09T00:00:00.000000000', '2023-02-10T00:00:00.000000000',\n       '2023-02-11T00:00:00.000000000', '2023-02-12T00:00:00.000000000',\n       '2023-02-13T00:00:00.000000000', '2023-02-14T00:00:00.000000000',\n       '2023-02-15T00:00:00.000000000', '2023-02-16T00:00:00.000000000',\n       '2023-02-17T00:00:00.000000000', '2023-02-18T00:00:00.000000000',\n       '2023-02-19T00:00:00.000000000', '2023-02-20T00:00:00.000000000',\n       '2023-02-21T00:00:00.000000000', '2023-02-22T00:00:00.000000000',\n       '2023-02-23T00:00:00.000000000', '2023-02-24T00:00:00.000000000',\n       '2023-02-25T00:00:00.000000000', '2023-02-26T00:00:00.000000000',\n       '2023-02-27T00:00:00.000000000', '2023-02-28T00:00:00.000000000',\n       '2023-03-01T00:00:00.000000000', '2023-03-02T00:00:00.000000000',\n       '2023-03-03T00:00:00.000000000', '2023-03-04T00:00:00.000000000',\n       '2023-03-05T00:00:00.000000000', '2023-03-06T00:00:00.000000000',\n       '2023-03-07T00:00:00.000000000', '2023-03-08T00:00:00.000000000',\n       '2023-03-09T00:00:00.000000000', '2023-03-10T00:00:00.000000000',\n       '2023-03-11T00:00:00.000000000', '2023-03-12T00:00:00.000000000',\n       '2023-03-13T00:00:00.000000000', '2023-03-14T00:00:00.000000000',\n       '2023-03-15T00:00:00.000000000', '2023-03-16T00:00:00.000000000',\n       '2023-03-17T00:00:00.000000000', '2023-03-18T00:00:00.000000000',\n       '2023-03-19T00:00:00.000000000', '2023-03-20T00:00:00.000000000',\n       '2023-03-21T00:00:00.000000000', '2023-03-22T00:00:00.000000000',\n       '2023-03-23T00:00:00.000000000', '2023-03-24T00:00:00.000000000',\n       '2023-03-25T00:00:00.000000000', '2023-03-26T00:00:00.000000000',\n       '2023-03-27T00:00:00.000000000', '2023-03-28T00:00:00.000000000',\n       '2023-03-29T00:00:00.000000000', '2023-03-30T00:00:00.000000000',\n       '2023-03-31T00:00:00.000000000', '2023-04-01T00:00:00.000000000',\n       '2023-04-02T00:00:00.000000000', '2023-04-03T00:00:00.000000000',\n       '2023-04-04T00:00:00.000000000', '2023-04-05T00:00:00.000000000',\n       '2023-04-06T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (10)precipitation(time, lon, lat)float32dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;units :mm/daylong_name :Daily mean precipitation rate (combined microwave-IR) estimate. Formerly precipitationCal.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.36 GiB\n24.72 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nprecipitation_cnt\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of all valid half-hourly precipitation retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nprecipitation_cnt_cond\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of half-hourly precipitation retrievals for the day where precipitation is at least 0.01 mm/hr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation\n\n\n(time, lon, lat)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm/day\n\nlong_name :\n\nDaily mean High Quality precipitation rate from all available microwave sources. Formerly HQprecipitation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.36 GiB\n24.72 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation_cnt\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of all valid half-hourly MWprecipitation retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation_cnt_cond\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of half-hourly MWprecipitation retrievals for the day where precipitation is at least 0.01 mm/hr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nrandomError\n\n\n(time, lon, lat)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm/day\n\nlong_name :\n\nRoot-mean-square error estimate for combined microwave-IR daily precipitation rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.36 GiB\n24.72 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nrandomError_cnt\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of valid half-hourly randomError retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nprobabilityLiquidPrecipitation\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\npercent\n\nlong_name :\n\nProbability of liquid precipitation\n\ndescription :\n\nProbability of liquid precipitation estimated with a diagnostic parameterization using ancillary data. 0=missing values; 1=likely solid; 100=likely liquid or no precipitation. Screen by positive precipitation or precipitation_cnt_cond to locate meaningful probabilities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntime_bnds\n\n\n(time, nv)\n\n\ndatetime64[ns]\n\n\ndask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\ntime nv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.17 kiB\n16 B\n\n\nShape\n(139, 2)\n(1, 2)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)lonPandasIndexPandasIndex(Float64Index([ -179.9499969482422, -179.85000610351562,             -179.75,\n              -179.64999389648438,  -179.5500030517578,  -179.4499969482422,\n              -179.35000610351562,             -179.25, -179.14999389648438,\n               -179.0500030517578,\n              ...\n                179.0500030517578,  179.14999389648438,              179.25,\n               179.35000610351562,   179.4499969482422,   179.5500030517578,\n               179.64999389648438,              179.75,  179.85000610351562,\n                179.9499969482422],\n             dtype='float64', name='lon', length=3600))latPandasIndexPandasIndex(Float64Index([            -89.95, -89.85000000000001,             -89.75,\n                          -89.65,             -89.55,             -89.45,\n              -89.35000000000001,             -89.25,             -89.15,\n                          -89.05,\n              ...\n                           89.05,  89.15000000000002,  89.25000000000001,\n               89.35000000000001,              89.45,              89.55,\n               89.65000000000002,  89.75000000000001,  89.85000000000001,\n                           89.95],\n             dtype='float64', name='lat', length=1800))timePandasIndexPandasIndex(DatetimeIndex(['2022-11-19', '2022-11-20', '2022-11-21', '2022-11-22',\n               '2022-11-23', '2022-11-24', '2022-11-25', '2022-11-26',\n               '2022-11-27', '2022-11-28',\n               ...\n               '2023-03-28', '2023-03-29', '2023-03-30', '2023-03-31',\n               '2023-04-01', '2023-04-02', '2023-04-03', '2023-04-04',\n               '2023-04-05', '2023-04-06'],\n              dtype='datetime64[ns]', name='time', length=139, freq=None))Attributes: (9)BeginDate :2022-11-19BeginTime :00:00:00.000ZEndDate :2022-11-19EndTime :23:59:59.999ZFileHeader :StartGranuleDateTime=2022-11-19T00:00:00.000Z;\nStopGranuleDateTime=2022-11-19T23:59:59.999ZInputPointer :3B-HHR.MS.MRG.3IMERG.20221119-S000000-E002959.0000.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S003000-E005959.0030.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S010000-E012959.0060.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S013000-E015959.0090.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S020000-E022959.0120.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S023000-E025959.0150.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S030000-E032959.0180.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S033000-E035959.0210.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S040000-E042959.0240.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S043000-E045959.0270.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S050000-E052959.0300.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S053000-E055959.0330.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S060000-E062959.0360.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S063000-E065959.0390.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S070000-E072959.0420.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S073000-E075959.0450.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S080000-E082959.0480.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S083000-E085959.0510.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S090000-E092959.0540.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S093000-E095959.0570.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S100000-E102959.0600.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S103000-E105959.0630.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S110000-E112959.0660.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S113000-E115959.0690.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S120000-E122959.0720.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S123000-E125959.0750.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S130000-E132959.0780.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S133000-E135959.0810.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S140000-E142959.0840.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S143000-E145959.0870.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S150000-E152959.0900.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S153000-E155959.0930.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S160000-E162959.0960.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S163000-E165959.0990.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S170000-E172959.1020.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S173000-E175959.1050.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S180000-E182959.1080.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S183000-E185959.1110.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S190000-E192959.1140.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S193000-E195959.1170.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S200000-E202959.1200.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S203000-E205959.1230.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S210000-E212959.1260.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S213000-E215959.1290.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S220000-E222959.1320.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S223000-E225959.1350.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S230000-E232959.1380.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S233000-E235959.1410.V07A.HDF5title :GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 degree (GPM_3IMERGDF)DOI :10.5067/GPM/IMERGDF/DAY/07ProductionTime :2023-08-25T14:03:25.792Z\n\n\n\n\nsearch_params = {\n    \"concept_id\": \"C2408009906-LPCLOUD\", # CMR concept ID for EMITL1BRAD.001\n    #\"day_night_flag\": \"day\",\n    \"cloud_cover\": (0, 10),\n    \"temporal\": (\"2022-05\", \"2023-08\"),\n    \"bounding_box\": (-99.65, 18.85, -98.5, 19.95)\n}\nresults = earthaccess.search_data(**search_params)"
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#additional-resources",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#additional-resources",
    "title": "NASA Earthdata Cloud Clinic",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nTutorials\nThis clinic was based off of several notebook tutorials including those presented during past workshop events, along with other materials co-created by the NASA Openscapes mentors: * 2021 Earthdata Cloud Hackathon * 2021 AGU Workshop * Accessing and working with ICESat-2 data in the cloud * Analyzing Sea Level Rise Using Earth Data in the Cloud\n\n\nCloud services\nThe examples used in the clinic provide an abbreviated and simplified workflow to explore access and subsetting options available through the Earthdata Cloud. There are several other options that can be used to interact with data in the Earthdata Cloud including:\n\nOPeNDAP\n\nHyrax provides direct access to subsetting of NASA data using Python or your favorite analysis tool\nTutorial highlighting OPeNDAP usage: https://nasa-openscapes.github.io/earthdata-cloud-cookbook/how-tos/working-locally/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html\n\nZarr-EOSDIS-Store\n\nThe zarr-eosdis-store library allows NASA EOSDIS Collections to be accessed efficiently by the Zarr Python library, provided they have a sidecar DMR++ metadata file generated.\nTutorial highlighting this library’s usage: https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/09_Zarr_Access.html\n\n\n\n\nSupport\n\nEarthdata Forum\n\nUser Services and community support for all things NASA Earthdata, including Earthdata Cloud\n\nEarthdata Webinar series\n\nWebinars from DAACs and other groups across EOSDIS including guidance on working with Earthdata Cloud\nSee the Earthdata YouTube channel for more videos"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The Enabling Analysis in the Cloud Using NASA Earth Science Data will take place on Sunday, December 10 from 13:00-16:30.\nZoom links will be shared directly with this group via a (calendar) meeting invite.\nNote, hands-on exercises will be executed from a Jupyter Lab instance in 2i2c Click here to spin up the instance and simultaneously clone this GitHub repository to follow along with the tutorials. Please pass along your GitHub Username to get access."
  },
  {
    "objectID": "schedule.html#workshop-schedule",
    "href": "schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\nDecember 10, 2023\n\n\n\nTime, PST (UTC-7)\nEvent\nLeads/Instructors\n\n\n\n\n1:00 pm\nWelcome / Open Science Mindset\nCassie Nickles (PO.DAAC)\n\n\n1:20 pm\nIntroduction to NASA Earthdata Cloud\nMichele Thornton (ORNL DAAC)\n\n\n1:40 pm\nOrientation and setup in JupyterHub in the Cloud\nMichele Thornton (ORNL DAAC)\n\n\n2:00 pm\nTutorial: In-cloud Science Workflow: Search & Discovery\nAaron Friesz (LP DAAC)\n\n\n2:30 pm\nBreak\n\n\n\n2:45 pm\nTutorial: In-cloud Science Workflow: Access, Subset & Plot\nDanny Kaufman (ASDC DAAC)\n\n\n3:45 pm\nQ & A\nAll NASA Openscapes Mentors\n\n\n4:15 pm\nClosing Survey\nCassie Nickles (PO.DAAC)"
  },
  {
    "objectID": "schedule.html#closing---close-out-your-jupyter-hub",
    "href": "schedule.html#closing---close-out-your-jupyter-hub",
    "title": "Schedule",
    "section": "Closing - Close out your Jupyter Hub!",
    "text": "Closing - Close out your Jupyter Hub!\n\nClose out your JupyterHub instance if you are finished for the day, following these instructions.\nYou will continue to have access to the 2i2c JupyterHub in AWS for two weeks following this Workshop. You may use that time to continue work and all learn more about migrating data access routines and science workflows to the Cloud. This cloud compute environment is supported by the NASA Openscapes project.\n\n\nThank you!"
  }
]