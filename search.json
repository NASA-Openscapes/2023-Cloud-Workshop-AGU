[
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up. Please remember your username and password!\n\n\n\nA GitHub account is required to gain access to the provided 2i2c cloud computing platform. Please visit https://github.com/join to register and create a free GitHub account. Make sure you have sent your username to the workshop leaders via this Google Form once you have done so for access to our cloud computing environment.\n\n\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2."
  },
  {
    "objectID": "prerequisites.html#before-the-workshop-you-will-need-the-following",
    "href": "prerequisites.html#before-the-workshop-you-will-need-the-following",
    "title": "Prerequisites",
    "section": "",
    "text": "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up. Please remember your username and password!\n\n\n\nA GitHub account is required to gain access to the provided 2i2c cloud computing platform. Please visit https://github.com/join to register and create a free GitHub account. Make sure you have sent your username to the workshop leaders via this Google Form once you have done so for access to our cloud computing environment.\n\n\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2."
  },
  {
    "objectID": "tutorials/Earthdata_Subset_and_Plot.html",
    "href": "tutorials/Earthdata_Subset_and_Plot.html",
    "title": "Part 2 of the In-cloud Science Workflow: Data subsetting and plotting with earthaccess, xarray, and harmony",
    "section": "",
    "text": "Welcome to Part 2 of the In-cloud Science Workflow workshop.\nIn these examples we will use the xarray, earthaccess, and harmony-py libraries to subset data and make figures using cartopy, matplotlib, and geoviews.\nWe will go through two examples of subsetting and plotting data in the Earthdata Cloud:\n\nExample 1 - earthaccess and xarray for precipitation estimates from IMERG, Daily Level 3 data\nExample 2 - harmony-py for direct cloud subsetting of precipitable water data from the DSCOVR EPIC Composite.\nAppendix 1 - Snow cover data from MODIS/Terra, Daily Level 3 data with rioxarray\nAppendix 2 - Snow mass data from SMAP, 3-hourly Level 4 data\n\nIn the first example, we will be accessing data directly from Amazon Web Services (AWS), specifically in the us-west-2 region, which is where all cloud-hosted NASA Earthdata reside. This shared compute environment (JupyterHub) is also running in the same location. We will then load the data into Python as an xarray dataset and use xarray to subset.\nIn the second example, we will demonstrate an example of pulling data via the cloud from an existing on-premise data server. In this example, the data are subsetted using one of the data transformation services provided in the NASA Earthdata system. Both xarray and harmony-py can be run outside of AWS as well.\nSee the bottom of the notebook for additional resources, including several tutorials that that served as a foundation for this clinic. Includes: https://github.com/rupesh2/atmospheric_rivers/tree/main\nNote: “direct cloud access” is also called “direct S3 access” or simply “direct access”."
  },
  {
    "objectID": "tutorials/Earthdata_Subset_and_Plot.html#summary",
    "href": "tutorials/Earthdata_Subset_and_Plot.html#summary",
    "title": "Part 2 of the In-cloud Science Workflow: Data subsetting and plotting with earthaccess, xarray, and harmony",
    "section": "",
    "text": "Welcome to Part 2 of the In-cloud Science Workflow workshop.\nIn these examples we will use the xarray, earthaccess, and harmony-py libraries to subset data and make figures using cartopy, matplotlib, and geoviews.\nWe will go through two examples of subsetting and plotting data in the Earthdata Cloud:\n\nExample 1 - earthaccess and xarray for precipitation estimates from IMERG, Daily Level 3 data\nExample 2 - harmony-py for direct cloud subsetting of precipitable water data from the DSCOVR EPIC Composite.\nAppendix 1 - Snow cover data from MODIS/Terra, Daily Level 3 data with rioxarray\nAppendix 2 - Snow mass data from SMAP, 3-hourly Level 4 data\n\nIn the first example, we will be accessing data directly from Amazon Web Services (AWS), specifically in the us-west-2 region, which is where all cloud-hosted NASA Earthdata reside. This shared compute environment (JupyterHub) is also running in the same location. We will then load the data into Python as an xarray dataset and use xarray to subset.\nIn the second example, we will demonstrate an example of pulling data via the cloud from an existing on-premise data server. In this example, the data are subsetted using one of the data transformation services provided in the NASA Earthdata system. Both xarray and harmony-py can be run outside of AWS as well.\nSee the bottom of the notebook for additional resources, including several tutorials that that served as a foundation for this clinic. Includes: https://github.com/rupesh2/atmospheric_rivers/tree/main\nNote: “direct cloud access” is also called “direct S3 access” or simply “direct access”."
  },
  {
    "objectID": "tutorials/Earthdata_Subset_and_Plot.html#learning-objectives",
    "href": "tutorials/Earthdata_Subset_and_Plot.html#learning-objectives",
    "title": "Part 2 of the In-cloud Science Workflow: Data subsetting and plotting with earthaccess, xarray, and harmony",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nExtract variables, temporal slices, and spatial slices from an xarray dataset\nPlot data and exclude data points via boolean conditions, using xarray, cartopy, matplotlib, and rasterio\nPlot a polygon geojson file with a basemap using geoviews\nConceptualize data subsetting services provided by NASA Earthdata, including Harmony\nUtilize the harmony-py library to request data over the Bay of San Francisco\n\n\nImport Required Packages\n\n# Suppress warnings\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\nfrom pprint import pprint\n\n# Example 1 imports\nimport earthaccess\nimport xarray as xr\nxr.set_options(display_expand_attrs=False)\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\n# Example 2 imports (Example 1 imports plus these...)\nimport datetime as dt\nimport json\nfrom cartopy.mpl.ticker import LatitudeFormatter, LongitudeFormatter\nimport geopandas as gpd\nimport geoviews as gv\ngv.extension('bokeh', 'matplotlib', logo=False)\nfrom harmony import Client, Collection, Request, CapabilitiesRequest\n\n# Appendix 1 imports\nfrom pathlib import Path\nimport rioxarray as rxr\n\n%matplotlib inline"
  },
  {
    "objectID": "tutorials/Earthdata_Subset_and_Plot.html#picking-up-where-we-left-off",
    "href": "tutorials/Earthdata_Subset_and_Plot.html#picking-up-where-we-left-off",
    "title": "Part 2 of the In-cloud Science Workflow: Data subsetting and plotting with earthaccess, xarray, and harmony",
    "section": "Picking up where we left off",
    "text": "Picking up where we left off\nWe will authenticate our Earthaccess session, and then open the results like we did in the Search & Discovery section.\n\nauth = earthaccess.login()\n# are we authenticated?\nif not auth.authenticated:\n    # ask for credentials and persist them in a .netrc file\n    auth.login(strategy=\"interactive\", persist=True)\n\nEARTHDATA_USERNAME and EARTHDATA_PASSWORD are not set in the current environment, try setting them or use a different strategy (netrc, interactive)\nYou're now authenticated with NASA Earthdata Login\nUsing token with expiration date: 01/26/2024\nUsing .netrc file for EDL"
  },
  {
    "objectID": "tutorials/Earthdata_Subset_and_Plot.html#example-1---xarray-subsetting---precipitation-estimates-from-imerg-daily-level-3",
    "href": "tutorials/Earthdata_Subset_and_Plot.html#example-1---xarray-subsetting---precipitation-estimates-from-imerg-daily-level-3",
    "title": "Part 2 of the In-cloud Science Workflow: Data subsetting and plotting with earthaccess, xarray, and harmony",
    "section": "Example 1 - Xarray Subsetting - Precipitation estimates from IMERG, Daily Level 3",
    "text": "Example 1 - Xarray Subsetting - Precipitation estimates from IMERG, Daily Level 3\n\nDataset\nWe will use the GPM IMERG Final Precipitation L3 Daily dataset for this tutorial. The IMERG Precipitation Rate provides the rain and snow rates in millimeters per hour (mm/hr). It is estimated by the Integrated Multi-satellitE Retrievals for Global Precipitation Measurement (GPM) (IMERG) algorithm. The IMERG algorithm uses passive-microwave data from the GPM constellation of satellites and infrared data from geosynchronous satellites. IMERG “morphs” observations to earlier or later times using wind from weather-model analyses. The daily IMERG dataset is derived from the half-hourly GPM_3IMERGHH. The derived result represents the final estimate of the daily mean precipitation rate in mm/day.\nThe IMERG data has 0.1 x 0.1 degree latitude-longitude resolution (approximately 11 by 11 km at the Equator). The grid covers the globe, although precipitation cannot always be estimated near the Poles. The dataset and algorithm are described in the data user guide and the Algorithm Theoretical Basis Document (ATBD).\nPlease cite the dataset as: &gt; Huffman, G.J., E.F. Stocker, D.T. Bolvin, E.J. Nelkin, Jackson Tan (2023), GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 degree V07, Edited by Andrey Savtchenko, Greenbelt, MD, Goddard Earth Sciences Data and Information Services Center (GES DISC), https://doi.org/10.5067/GPM/IMERGDF/DAY/07\n\ncollection_id = 'C2723754864-GES_DISC'  # GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 degree V07 (GPM_3IMERGDF)\n\n# Bounds within which we search for data granules\ndate_start = \"2023-02-24\"\ndate_end = \"2023-02-26\"\ndate_range = (date_start, date_end)\nbbox = (-127.0761, 31.6444, -113.9039, 42.6310)  # min lon, min lat, max lon, max lat\n\n# For reference (e.g., to visualize in https://geojson.io/), here is a GeoJSON representing the above bounding box:\n# {\"type\": \"FeatureCollection\", \"features\": [{\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"LineString\", \"bbox\": [-127.0761, 31.6444, -113.9039, 42.631], \"coordinates\": [[-113.9039, 42.631], [-127.0761,42.631], [-127.0761, 31.6444], [-113.9039, 31.6444], [-113.9039, 42.631]]}}]}\n\nresults = earthaccess.search_data(\n    concept_id = collection_id,\n    cloud_hosted = True,\n    temporal = date_range,\n    bounding_box = bbox,\n)\n\nds = xr.open_mfdataset(earthaccess.open(results))\n\nGranules found: 3\n Opening 3 granules, approx size: 0.08 GB\n\n\n\n\n\n\n\n\n\n\n\nNote that xarray works with “lazy” computation whenever possible. In this case, the metadata are loaded into JupyterHub memory, but the data arrays and their values are not — until there is a need for them.\nLet’s print out all the variable names.\n\nfor v in ds.variables:\n    print(v)\n\nprecipitation\nprecipitation_cnt\nprecipitation_cnt_cond\nMWprecipitation\nMWprecipitation_cnt\nMWprecipitation_cnt_cond\nrandomError\nrandomError_cnt\nprobabilityLiquidPrecipitation\nlon\nlat\ntime\ntime_bnds\n\n\nOf the variables listed above, we are interested in three variables: precipitation, precipitation_cnt_cond, and probabilityLiquidPrecipitation. Let’s print their attributes.\n\nds.variables['precipitation'].attrs\n\n{'units': 'mm/day',\n 'long_name': 'Daily mean precipitation rate (combined microwave-IR) estimate. Formerly precipitationCal.'}\n\n\n\nds.variables['precipitation_cnt_cond'].attrs\n\n{'units': 'count',\n 'long_name': 'Count of half-hourly precipitation retrievals for the day where precipitation is at least 0.01 mm/hr'}\n\n\n\nds.variables['probabilityLiquidPrecipitation'].attrs\n\n{'units': 'percent',\n 'long_name': 'Probability of liquid precipitation',\n 'description': 'Probability of liquid precipitation estimated with a diagnostic parameterization using ancillary data. 0=missing values; 1=likely solid; 100=likely liquid or no precipitation.  Screen by positive precipitation or precipitation_cnt_cond to locate meaningful probabilities.'}\n\n\n\n\nSubsetting\nIn addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection/regridding, and file aggregation. What does subsetting mean? To subset means to extract only the portions of a dataset that are needed for a given purpose. Here’s a generalized graphic of what we mean.\n\nThere are three primary types of subsetting that we will walk through: 1. Temporal 2. Spatial 3. Variable\nIn each case, we will be excluding parts of the dataset that are not wanted using xarray. Note that “subsetting” is also called a data “transformation”.\n\nds.time.values\n\narray(['2023-02-24T00:00:00.000000000', '2023-02-25T00:00:00.000000000',\n       '2023-02-26T00:00:00.000000000'], dtype='datetime64[ns]')\n\n\nWe start with a subset that represents the U.S. state of California. Notice the dimensions of the Dataset and each variable — time, lon, lat, and ‘nv’ (number of vertices) for the bounds variable.\n\n# Display the full dataset's metadata\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                         (time: 3, lon: 3600, lat: 1800, nv: 2)\nCoordinates:\n  * lon                             (lon) float32 -179.9 -179.9 ... 179.9 179.9\n  * lat                             (lat) float64 -89.95 -89.85 ... 89.85 89.95\n  * time                            (time) datetime64[ns] 2023-02-24 ... 2023...\nDimensions without coordinates: nv\nData variables:\n    precipitation                   (time, lon, lat) float32 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    precipitation_cnt               (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    precipitation_cnt_cond          (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    MWprecipitation                 (time, lon, lat) float32 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    MWprecipitation_cnt             (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    MWprecipitation_cnt_cond        (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    randomError                     (time, lon, lat) float32 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    randomError_cnt                 (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    probabilityLiquidPrecipitation  (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    time_bnds                       (time, nv) datetime64[ns] dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\nAttributes: (9)xarray.DatasetDimensions:time: 3lon: 3600lat: 1800nv: 2Coordinates: (3)lon(lon)float32-179.9 -179.9 ... 179.9 179.9units :degrees_eastlong_name :Longitudearray([-179.95, -179.85, -179.75, ...,  179.75,  179.85,  179.95],\n      dtype=float32)lat(lat)float64-89.95 -89.85 ... 89.85 89.95units :degrees_northlong_name :Latitudearray([-89.95, -89.85, -89.75, ...,  89.75,  89.85,  89.95])time(time)datetime64[ns]2023-02-24 2023-02-25 2023-02-26standard_name :timelong_name :timebounds :time_bndsarray(['2023-02-24T00:00:00.000000000', '2023-02-25T00:00:00.000000000',\n       '2023-02-26T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (10)precipitation(time, lon, lat)float32dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;units :mm/daylong_name :Daily mean precipitation rate (combined microwave-IR) estimate. Formerly precipitationCal.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.16 MiB\n24.72 MiB\n\n\nShape\n(3, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nprecipitation_cnt\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of all valid half-hourly precipitation retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.54 MiB\n6.18 MiB\n\n\nShape\n(3, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nprecipitation_cnt_cond\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of half-hourly precipitation retrievals for the day where precipitation is at least 0.01 mm/hr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.54 MiB\n6.18 MiB\n\n\nShape\n(3, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation\n\n\n(time, lon, lat)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm/day\n\nlong_name :\n\nDaily mean High Quality precipitation rate from all available microwave sources. Formerly HQprecipitation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.16 MiB\n24.72 MiB\n\n\nShape\n(3, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation_cnt\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of all valid half-hourly MWprecipitation retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.54 MiB\n6.18 MiB\n\n\nShape\n(3, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation_cnt_cond\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of half-hourly MWprecipitation retrievals for the day where precipitation is at least 0.01 mm/hr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.54 MiB\n6.18 MiB\n\n\nShape\n(3, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nrandomError\n\n\n(time, lon, lat)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm/day\n\nlong_name :\n\nRoot-mean-square error estimate for combined microwave-IR daily precipitation rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.16 MiB\n24.72 MiB\n\n\nShape\n(3, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nrandomError_cnt\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of valid half-hourly randomError retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.54 MiB\n6.18 MiB\n\n\nShape\n(3, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nprobabilityLiquidPrecipitation\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\npercent\n\nlong_name :\n\nProbability of liquid precipitation\n\ndescription :\n\nProbability of liquid precipitation estimated with a diagnostic parameterization using ancillary data. 0=missing values; 1=likely solid; 100=likely liquid or no precipitation. Screen by positive precipitation or precipitation_cnt_cond to locate meaningful probabilities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.54 MiB\n6.18 MiB\n\n\nShape\n(3, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntime_bnds\n\n\n(time, nv)\n\n\ndatetime64[ns]\n\n\ndask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\ntime nv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n48 B\n16 B\n\n\nShape\n(3, 2)\n(1, 2)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)lonPandasIndexPandasIndex(Float64Index([ -179.9499969482422, -179.85000610351562,             -179.75,\n              -179.64999389648438,  -179.5500030517578,  -179.4499969482422,\n              -179.35000610351562,             -179.25, -179.14999389648438,\n               -179.0500030517578,\n              ...\n                179.0500030517578,  179.14999389648438,              179.25,\n               179.35000610351562,   179.4499969482422,   179.5500030517578,\n               179.64999389648438,              179.75,  179.85000610351562,\n                179.9499969482422],\n             dtype='float64', name='lon', length=3600))latPandasIndexPandasIndex(Float64Index([            -89.95, -89.85000000000001,             -89.75,\n                          -89.65,             -89.55,             -89.45,\n              -89.35000000000001,             -89.25,             -89.15,\n                          -89.05,\n              ...\n                           89.05,  89.15000000000002,  89.25000000000001,\n               89.35000000000001,              89.45,              89.55,\n               89.65000000000002,  89.75000000000001,  89.85000000000001,\n                           89.95],\n             dtype='float64', name='lat', length=1800))timePandasIndexPandasIndex(DatetimeIndex(['2023-02-24', '2023-02-25', '2023-02-26'], dtype='datetime64[ns]', name='time', freq=None))Attributes: (9)BeginDate :2023-02-24BeginTime :00:00:00.000ZEndDate :2023-02-24EndTime :23:59:59.999ZFileHeader :StartGranuleDateTime=2023-02-24T00:00:00.000Z;\nStopGranuleDateTime=2023-02-24T23:59:59.999ZInputPointer :3B-HHR.MS.MRG.3IMERG.20230224-S000000-E002959.0000.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S003000-E005959.0030.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S010000-E012959.0060.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S013000-E015959.0090.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S020000-E022959.0120.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S023000-E025959.0150.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S030000-E032959.0180.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S033000-E035959.0210.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S040000-E042959.0240.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S043000-E045959.0270.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S050000-E052959.0300.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S053000-E055959.0330.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S060000-E062959.0360.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S063000-E065959.0390.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S070000-E072959.0420.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S073000-E075959.0450.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S080000-E082959.0480.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S083000-E085959.0510.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S090000-E092959.0540.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S093000-E095959.0570.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S100000-E102959.0600.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S103000-E105959.0630.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S110000-E112959.0660.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S113000-E115959.0690.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S120000-E122959.0720.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S123000-E125959.0750.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S130000-E132959.0780.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S133000-E135959.0810.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S140000-E142959.0840.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S143000-E145959.0870.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S150000-E152959.0900.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S153000-E155959.0930.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S160000-E162959.0960.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S163000-E165959.0990.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S170000-E172959.1020.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S173000-E175959.1050.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S180000-E182959.1080.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S183000-E185959.1110.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S190000-E192959.1140.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S193000-E195959.1170.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S200000-E202959.1200.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S203000-E205959.1230.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S210000-E212959.1260.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S213000-E215959.1290.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S220000-E222959.1320.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S223000-E225959.1350.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S230000-E232959.1380.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S233000-E235959.1410.V07A.HDF5title :GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 degree (GPM_3IMERGDF)DOI :10.5067/GPM/IMERGDF/DAY/07ProductionTime :2023-08-29T08:52:09.517Z\n\n\nNow we will prepare a subset. We’re using essentially the same spatial bounds as above; however, as opposed to the earthaccess inputs above, here we must provide inputs in the formats expected by xarray. Instead of a single, four-element, bounding box, we use Python slice objects, which are defined by starting and ending numbers.\n\nds_subset = ds.sel(time='2023-02-24', lat=slice(31, 43), lon=slice(-125, -113)) \nds_subset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                         (lon: 120, lat: 120, nv: 2)\nCoordinates:\n  * lon                             (lon) float32 -124.9 -124.8 ... -113.1\n  * lat                             (lat) float64 31.05 31.15 ... 42.85 42.95\n    time                            datetime64[ns] 2023-02-24\nDimensions without coordinates: nv\nData variables:\n    precipitation                   (lon, lat) float32 dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n    precipitation_cnt               (lon, lat) int8 dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n    precipitation_cnt_cond          (lon, lat) int8 dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n    MWprecipitation                 (lon, lat) float32 dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n    MWprecipitation_cnt             (lon, lat) int8 dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n    MWprecipitation_cnt_cond        (lon, lat) int8 dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n    randomError                     (lon, lat) float32 dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n    randomError_cnt                 (lon, lat) int8 dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n    probabilityLiquidPrecipitation  (lon, lat) int8 dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n    time_bnds                       (nv) datetime64[ns] dask.array&lt;chunksize=(2,), meta=np.ndarray&gt;\nAttributes: (9)xarray.DatasetDimensions:lon: 120lat: 120nv: 2Coordinates: (3)lon(lon)float32-124.9 -124.8 ... -113.2 -113.1units :degrees_eastlong_name :Longitudearray([-124.95, -124.85, -124.75, -124.65, -124.55, -124.45, -124.35, -124.25,\n       -124.15, -124.05, -123.95, -123.85, -123.75, -123.65, -123.55, -123.45,\n       -123.35, -123.25, -123.15, -123.05, -122.95, -122.85, -122.75, -122.65,\n       -122.55, -122.45, -122.35, -122.25, -122.15, -122.05, -121.95, -121.85,\n       -121.75, -121.65, -121.55, -121.45, -121.35, -121.25, -121.15, -121.05,\n       -120.95, -120.85, -120.75, -120.65, -120.55, -120.45, -120.35, -120.25,\n       -120.15, -120.05, -119.95, -119.85, -119.75, -119.65, -119.55, -119.45,\n       -119.35, -119.25, -119.15, -119.05, -118.95, -118.85, -118.75, -118.65,\n       -118.55, -118.45, -118.35, -118.25, -118.15, -118.05, -117.95, -117.85,\n       -117.75, -117.65, -117.55, -117.45, -117.35, -117.25, -117.15, -117.05,\n       -116.95, -116.85, -116.75, -116.65, -116.55, -116.45, -116.35, -116.25,\n       -116.15, -116.05, -115.95, -115.85, -115.75, -115.65, -115.55, -115.45,\n       -115.35, -115.25, -115.15, -115.05, -114.95, -114.85, -114.75, -114.65,\n       -114.55, -114.45, -114.35, -114.25, -114.15, -114.05, -113.95, -113.85,\n       -113.75, -113.65, -113.55, -113.45, -113.35, -113.25, -113.15, -113.05],\n      dtype=float32)lat(lat)float6431.05 31.15 31.25 ... 42.85 42.95units :degrees_northlong_name :Latitudearray([31.05, 31.15, 31.25, 31.35, 31.45, 31.55, 31.65, 31.75, 31.85, 31.95,\n       32.05, 32.15, 32.25, 32.35, 32.45, 32.55, 32.65, 32.75, 32.85, 32.95,\n       33.05, 33.15, 33.25, 33.35, 33.45, 33.55, 33.65, 33.75, 33.85, 33.95,\n       34.05, 34.15, 34.25, 34.35, 34.45, 34.55, 34.65, 34.75, 34.85, 34.95,\n       35.05, 35.15, 35.25, 35.35, 35.45, 35.55, 35.65, 35.75, 35.85, 35.95,\n       36.05, 36.15, 36.25, 36.35, 36.45, 36.55, 36.65, 36.75, 36.85, 36.95,\n       37.05, 37.15, 37.25, 37.35, 37.45, 37.55, 37.65, 37.75, 37.85, 37.95,\n       38.05, 38.15, 38.25, 38.35, 38.45, 38.55, 38.65, 38.75, 38.85, 38.95,\n       39.05, 39.15, 39.25, 39.35, 39.45, 39.55, 39.65, 39.75, 39.85, 39.95,\n       40.05, 40.15, 40.25, 40.35, 40.45, 40.55, 40.65, 40.75, 40.85, 40.95,\n       41.05, 41.15, 41.25, 41.35, 41.45, 41.55, 41.65, 41.75, 41.85, 41.95,\n       42.05, 42.15, 42.25, 42.35, 42.45, 42.55, 42.65, 42.75, 42.85, 42.95])time()datetime64[ns]2023-02-24standard_name :timelong_name :timebounds :time_bndsarray('2023-02-24T00:00:00.000000000', dtype='datetime64[ns]')Data variables: (10)precipitation(lon, lat)float32dask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;units :mm/daylong_name :Daily mean precipitation rate (combined microwave-IR) estimate. Formerly precipitationCal.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n56.25 kiB\n56.25 kiB\n\n\nShape\n(120, 120)\n(120, 120)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nprecipitation_cnt\n\n\n(lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of all valid half-hourly precipitation retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n14.06 kiB\n14.06 kiB\n\n\nShape\n(120, 120)\n(120, 120)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nprecipitation_cnt_cond\n\n\n(lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of half-hourly precipitation retrievals for the day where precipitation is at least 0.01 mm/hr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n14.06 kiB\n14.06 kiB\n\n\nShape\n(120, 120)\n(120, 120)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation\n\n\n(lon, lat)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm/day\n\nlong_name :\n\nDaily mean High Quality precipitation rate from all available microwave sources. Formerly HQprecipitation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n56.25 kiB\n56.25 kiB\n\n\nShape\n(120, 120)\n(120, 120)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation_cnt\n\n\n(lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of all valid half-hourly MWprecipitation retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n14.06 kiB\n14.06 kiB\n\n\nShape\n(120, 120)\n(120, 120)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation_cnt_cond\n\n\n(lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of half-hourly MWprecipitation retrievals for the day where precipitation is at least 0.01 mm/hr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n14.06 kiB\n14.06 kiB\n\n\nShape\n(120, 120)\n(120, 120)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nrandomError\n\n\n(lon, lat)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm/day\n\nlong_name :\n\nRoot-mean-square error estimate for combined microwave-IR daily precipitation rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n56.25 kiB\n56.25 kiB\n\n\nShape\n(120, 120)\n(120, 120)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nrandomError_cnt\n\n\n(lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of valid half-hourly randomError retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n14.06 kiB\n14.06 kiB\n\n\nShape\n(120, 120)\n(120, 120)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nprobabilityLiquidPrecipitation\n\n\n(lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(120, 120), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\npercent\n\nlong_name :\n\nProbability of liquid precipitation\n\ndescription :\n\nProbability of liquid precipitation estimated with a diagnostic parameterization using ancillary data. 0=missing values; 1=likely solid; 100=likely liquid or no precipitation. Screen by positive precipitation or precipitation_cnt_cond to locate meaningful probabilities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n14.06 kiB\n14.06 kiB\n\n\nShape\n(120, 120)\n(120, 120)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntime_bnds\n\n\n(nv)\n\n\ndatetime64[ns]\n\n\ndask.array&lt;chunksize=(2,), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\ntime nv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n16 B\n16 B\n\n\nShape\n(2,)\n(2,)\n\n\nDask graph\n1 chunks in 8 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (2)lonPandasIndexPandasIndex(Float64Index([-124.94999694824219,  -124.8499984741211,             -124.75,\n               -124.6500015258789, -124.55000305175781, -124.44999694824219,\n               -124.3499984741211,             -124.25,  -124.1500015258789,\n              -124.05000305175781,\n              ...\n              -113.94999694824219,  -113.8499984741211,             -113.75,\n               -113.6500015258789, -113.55000305175781, -113.44999694824219,\n               -113.3499984741211,             -113.25,  -113.1500015258789,\n              -113.05000305175781],\n             dtype='float64', name='lon', length=120))latPandasIndexPandasIndex(Float64Index([             31.05,  31.15000000000001, 31.250000000000004,\n              31.350000000000012, 31.450000000000006,              31.55,\n               31.65000000000001, 31.750000000000004, 31.850000000000012,\n              31.950000000000006,\n              ...\n                           42.05,  42.14999999999999, 42.250000000000014,\n               42.35000000000001,              42.45,              42.55,\n               42.64999999999999, 42.750000000000014,  42.85000000000001,\n                           42.95],\n             dtype='float64', name='lat', length=120))Attributes: (9)BeginDate :2023-02-24BeginTime :00:00:00.000ZEndDate :2023-02-24EndTime :23:59:59.999ZFileHeader :StartGranuleDateTime=2023-02-24T00:00:00.000Z;\nStopGranuleDateTime=2023-02-24T23:59:59.999ZInputPointer :3B-HHR.MS.MRG.3IMERG.20230224-S000000-E002959.0000.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S003000-E005959.0030.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S010000-E012959.0060.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S013000-E015959.0090.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S020000-E022959.0120.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S023000-E025959.0150.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S030000-E032959.0180.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S033000-E035959.0210.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S040000-E042959.0240.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S043000-E045959.0270.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S050000-E052959.0300.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S053000-E055959.0330.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S060000-E062959.0360.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S063000-E065959.0390.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S070000-E072959.0420.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S073000-E075959.0450.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S080000-E082959.0480.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S083000-E085959.0510.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S090000-E092959.0540.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S093000-E095959.0570.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S100000-E102959.0600.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S103000-E105959.0630.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S110000-E112959.0660.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S113000-E115959.0690.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S120000-E122959.0720.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S123000-E125959.0750.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S130000-E132959.0780.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S133000-E135959.0810.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S140000-E142959.0840.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S143000-E145959.0870.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S150000-E152959.0900.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S153000-E155959.0930.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S160000-E162959.0960.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S163000-E165959.0990.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S170000-E172959.1020.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S173000-E175959.1050.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S180000-E182959.1080.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S183000-E185959.1110.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S190000-E192959.1140.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S193000-E195959.1170.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S200000-E202959.1200.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S203000-E205959.1230.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S210000-E212959.1260.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S213000-E215959.1290.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S220000-E222959.1320.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S223000-E225959.1350.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S230000-E232959.1380.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20230224-S233000-E235959.1410.V07A.HDF5title :GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 degree (GPM_3IMERGDF)DOI :10.5067/GPM/IMERGDF/DAY/07ProductionTime :2023-08-29T08:52:09.517Z\n\n\nNotice the differences?\n\n\nPlotting\nWe will first plot using the methods built-in to the xarray package.\nNote that, as opposed to the “lazy” loading of metadata previously, this will now perform “eager” computation, pulling the required data chunks.\n\nds_subset['precipitation'].plot(figsize=(10,6), x='lon', y='lat');\n\n\n\n\nNow let’s utilize the “Probability of liquid precipitation phase” (probabilityLiquidPrecipitation) variable to split apart the snow precipitation from everything else. From the variable’s description attribute, we can see that “0=missing values; 1=likely solid; 100=likely liquid or no precipitation”.\nMoreover, we’ll utilize precipitation_cnt_cond to filter out data points that had less than 0.01 mm/hr preciptation amounts.\n\nsnow = ds_subset['precipitation'].where(\n    (ds_subset.precipitation_cnt_cond&gt;0) & (ds_subset.probabilityLiquidPrecipitation == 1)\n)\n\nprcp = ds_subset['precipitation'].where(\n    (ds_subset.precipitation_cnt_cond&gt;0) & (ds_subset.probabilityLiquidPrecipitation != 1)\n)\n\nIn the following plotting commands, we utilize cartopy and matplotlib to generate a more customized figure.\ncartopy is used to set the map projection (to PlateCarree) and to add U.S. state boundary lines to the figure. matplotlib’s pcolormesh is used to generate the color plot, with colors determined by the third argument’s value.\n\n# create the plot\nproj = ccrs.PlateCarree()\nfig, ax = plt.subplots(figsize=(8,5), dpi=130, facecolor=\"w\", subplot_kw=dict(projection=proj))\n\nsnowax = plt.pcolormesh(prcp.lon, prcp.lat, snow.squeeze(), vmax=53, cmap='cool')\nprcpax = plt.pcolormesh(prcp.lon, prcp.lat, prcp.squeeze(), vmax=53, cmap='RdYlGn')\n\nplt.colorbar(snowax, ax=ax, label=\"snow (mm/day)\")\nplt.colorbar(prcpax, ax=ax, label=\"rainfall (mm/day)\")\nax.add_feature(cfeature.STATES)\nax.set_extent([-125, -113.0, 31.0, 43.0], crs=proj)\nax.set_title(f'Precipitation {date_start}')\n\nplt.show()\n\n\n\n\nNotice the enhancements?"
  },
  {
    "objectID": "tutorials/Earthdata_Subset_and_Plot.html#example-2---harmony-py-subsetting---precipitable-water-from-dscovr-epic-composite",
    "href": "tutorials/Earthdata_Subset_and_Plot.html#example-2---harmony-py-subsetting---precipitable-water-from-dscovr-epic-composite",
    "title": "Part 2 of the In-cloud Science Workflow: Data subsetting and plotting with earthaccess, xarray, and harmony",
    "section": "Example 2 - Harmony-py Subsetting - Precipitable Water from DSCOVR-EPIC Composite",
    "text": "Example 2 - Harmony-py Subsetting - Precipitable Water from DSCOVR-EPIC Composite\n\nDataset\nThe NASA Earth Polychromatic Imaging Camera (EPIC)-view Multi-Sensor Global Cloud and Radiance Composites are generated by optimally merging together multiple imagers on low Earth orbit (LEO) satellites (including MODIS, VIIRS, and AVHRR) and geostationary (GEO) satellites (including GOES-13 and -15, METEOSAT-7 and -10, MTSAT-2, and Himawari-8). These provide a seamless global composite product at 5-km resolution by using an aggregated rating that considers five parameters (nominal satellite resolution, pixel time relative to the Earth Polychromatic Imaging Camera (EPIC) observation time, viewing zenith angle, distance from day/night terminator, and sun glint factor) and selects the best observation at the time nearest to the EPIC measurements. The global composite data are then remapped into the EPIC Field of View (FOV) by convolving the high-resolution cloud properties with the EPIC point spread function (PSF) defined with a half-pixel accuracy to produce the EPIC composite. PSF-weighted radiances and cloud properties averages are computed separately for each cloud phase. Ancillary data (i.e., surface type, snow and ice map, skin temperature, precipitable water, etc.) needed for anisotropic factor selections are also included in the composite. These composite images are produced for each observation time of the EPIC instrument (typically 300 to 600 composites per month, or every 1 to 3 hours).\nThe dataset and development of the composite product is described in the Khlopenkov et al., 2017 and the Product Description page. This dataset can also be viewed in Earthdata Search.\nPlease cite the dataset as: &gt; NASA/LARC/SD/ASDC. (2017). EPIC-view satellite composites for DSCOVR, Version 1 [Data set]. NASA Langley Atmospheric Science Data Center DAAC. Retrieved from https://doi.org/10.5067/EPIC/DSCOVR/L2_COMPOSITE_01.\n\n\nHarmony\nHarmony is the behind-the-scenes orchestrator for much of the cloud-based transformations happening on NASA’s Earthdata Search interface. However, requests can also be sent directly to Harmony in a programmatic fashion, either through use of the harmony-py Python library or through transmitting underlying HTTP requests. In this example, we demonstrate the use of harmony-py, which was created as an alternative to Harmony’s RESTful Application Programming Interface (API) and to make it more convenient to invoke Harmony directly from a Python environment, such as Jupyter notebooks or larger Python applications.\nNote that additional examples can be found on the harmony-py GitHub page here.\nFirst we need to instantiate a Client object, with which we will be able to interact with Harmony.\n\nharmony_client = Client()\n\n\nInspecting a data collection for its capabilities and variables\nWith harmony-py, you can request a report of the capabilities that are configured for a given collection. We use that function here to inspect the DSCOVR EPIC-view Composite collection.\n\ncollection_id = \"C1576365803-LARC_ASDC\"  # EPIC-view satellite composites for DSCOVR, Version 1\n\ncapabilities_request = CapabilitiesRequest(collection_id=collection_id)\n\ncapabilities = harmony_client.submit(capabilities_request)\ncapabilities_str = json.dumps(capabilities, indent=2)\n\nThis data collection has one “service” associated with it, which provides several subsetting capabilities.\n\npprint(capabilities[\"services\"])\n\n[{'capabilities': {'output_formats': ['application/netcdf',\n                                      'application/x-netcdf4'],\n                   'subsetting': {'bbox': True,\n                                  'shape': True,\n                                  'temporal': True,\n                                  'variable': True}},\n  'href': 'https://cmr.earthdata.nasa.gov/search/concepts/S1962070864-POCLOUD',\n  'name': 'podaac/l2-subsetter'}]\n\n\nWe can also see the list of variables associated with this data collection.\n\npprint([v[\"name\"] for v in capabilities[\"variables\"]])\n\n['clear_sky/BT_10.8um',\n 'clear_sky/BT_12.0um',\n 'clear_sky/BT_3.75um',\n 'clear_sky/BT_6.75um',\n 'clear_sky/LW_broadband_flux',\n 'clear_sky/pixel_fraction',\n 'clear_sky/ref_0.63um',\n 'clear_sky/ref_0.86um',\n 'clear_sky/skin_temperature',\n 'clear_sky/SW_broadband_albedo',\n 'clear_sky/SW_broadband_albedo_EPIC',\n 'EPIC/band_680nm',\n 'EPIC/centroid_mean_latitude',\n 'EPIC/centroid_mean_longitude',\n 'EPIC/ndvi',\n 'EPIC/relative_azimuth',\n 'EPIC/solar_zenith',\n 'EPIC/view_zenith',\n 'general/lat',\n 'general/lon',\n 'general/MOA_skin_temperature',\n 'general/precipitable_water',\n 'general/relative_azimuth',\n 'general/relative_time',\n 'general/satellite_ID',\n 'general/solar_zenith',\n 'general/surface_type_fraction',\n 'general/surface_type_index',\n 'general/surface_U_wind',\n 'general/surface_V_wind',\n 'general/vertical_temperature_change',\n 'general/view_zenith',\n 'global_flux/albedo_epic',\n 'global_flux/albedo_hidden',\n 'global_flux/epic_view_mask',\n 'global_flux/insolation_epic',\n 'global_flux/insolation_hidden',\n 'global_flux/LW_epic',\n 'global_flux/LW_epic_dark',\n 'global_flux/LW_hidden',\n 'global_flux/mask_lat',\n 'global_flux/mask_lon',\n 'global_flux/SW_epic',\n 'global_flux/SW_hidden',\n 'granule_name_list',\n 'ice_cloud/BT_10.8um',\n 'ice_cloud/BT_12.0um',\n 'ice_cloud/BT_3.75um',\n 'ice_cloud/BT_6.75um',\n 'ice_cloud/cloud_eff_height',\n 'ice_cloud/cloud_eff_particle_size',\n 'ice_cloud/cloud_eff_pressure',\n 'ice_cloud/cloud_eff_temperature',\n 'ice_cloud/cloud_optical_depth',\n 'ice_cloud/cloud_top_height',\n 'ice_cloud/log_cloud_optical_depth',\n 'ice_cloud/LW_broadband_flux',\n 'ice_cloud/pixel_fraction',\n 'ice_cloud/ref_0.63um',\n 'ice_cloud/ref_0.86um',\n 'ice_cloud/SW_broadband_albedo',\n 'ice_cloud/SW_broadband_albedo_EPIC',\n 'no_retrieval/BT_10.8um',\n 'no_retrieval/BT_12.0um',\n 'no_retrieval/BT_3.75um',\n 'no_retrieval/BT_6.75um',\n 'no_retrieval/LW_broadband_flux',\n 'no_retrieval/pixel_fraction',\n 'no_retrieval/ref_0.63um',\n 'no_retrieval/ref_0.86um',\n 'no_retrieval/SW_broadband_albedo',\n 'reference_time_MJD',\n 'reference_time_stamp',\n 'total_cloud/BT_10.8um',\n 'total_cloud/BT_12.0um',\n 'total_cloud/BT_3.75um',\n 'total_cloud/BT_6.75um',\n 'total_cloud/cloud_eff_height',\n 'total_cloud/cloud_eff_particle_size',\n 'total_cloud/cloud_eff_pressure',\n 'total_cloud/cloud_eff_temperature',\n 'total_cloud/cloud_optical_depth',\n 'total_cloud/cloud_top_height',\n 'total_cloud/log_cloud_optical_depth',\n 'total_cloud/LW_broadband_flux',\n 'total_cloud/pixel_fraction',\n 'total_cloud/ref_0.63um',\n 'total_cloud/ref_0.86um',\n 'total_cloud/SW_broadband_albedo',\n 'water_cloud/BT_10.8um',\n 'water_cloud/BT_12.0um',\n 'water_cloud/BT_3.75um',\n 'water_cloud/BT_6.75um',\n 'water_cloud/cloud_eff_height',\n 'water_cloud/cloud_eff_particle_size',\n 'water_cloud/cloud_eff_pressure',\n 'water_cloud/cloud_eff_temperature',\n 'water_cloud/cloud_optical_depth',\n 'water_cloud/cloud_top_height',\n 'water_cloud/log_cloud_optical_depth',\n 'water_cloud/LW_broadband_flux',\n 'water_cloud/pixel_fraction',\n 'water_cloud/ref_0.63um',\n 'water_cloud/ref_0.86um',\n 'water_cloud/SW_broadband_albedo',\n 'water_cloud/SW_broadband_albedo_EPIC']\n\n\nThe subsetter service capabilities told us what the service is capable of “in general”. How about the capabilities reported for this data collection in particular?\n\npprint({key: val \n        for key, val in capabilities.items() \n        if key not in ['services', 'variables']}\n      )\n\n{'bboxSubset': True,\n 'capabilitiesVersion': '2',\n 'concatenate': False,\n 'conceptId': 'C1576365803-LARC_ASDC',\n 'outputFormats': ['application/netcdf', 'application/x-netcdf4'],\n 'reproject': False,\n 'shapeSubset': True,\n 'shortName': 'DSCOVR_EPIC_L2_COMPOSITE',\n 'variableSubset': True}\n\n\nNotice the Trues and the Falses?\n\n\n\nSubsetting\n\nDefine an area of interest\nFor this example, we will use a GeoJSON to specify a non-rectangular region instead of a simpler, rectangular bounding box. We will use the GeoJSON that defines a region around San Francisco.\n\n# Read the GeoJSON into a GeoDataFrame\nsf_geojson = '../../2023-Cloud-Workshop-AGU/data/sf_to_sierranvmt.geojson'\ngdf = gpd.read_file(sf_geojson)\n\nHere we illustrate the use of GeoViews, which is another open source data visualization library, like matplotlib. GeoViews is designed to work well with netCDF data, as well as Geopandas dataframes. The syntax for Geoviews is different in several ways — e.g., the dataset is often specified as the first argument and different components are combined using the * symbol.\n\n# We define a Geoview Point so we can visualize the area of interest in relation to San Francisco\nsf = (-122.42, 37.77, 'SanFrancisco')\ncities_lonlat = gv.Points([sf], vdims='City')\n\n# Generate an image\nbase = gv.tile_sources.EsriImagery.opts(width=650, height=500)\nocean_map = gv.Polygons(gdf).opts(line_color='yellow', line_width=5, color=None)\n\nbase * ocean_map * cities_lonlat.options(size=20, color='red', marker='x')\n\n\n\n\n\n  \n\n\n\n\n\n\nBuild a Harmony subsetting request\nA Harmony request can include spatial, temporal, and variable subsetting all in the same request. Here we will request all three types of subsetting to be performed on the EPIC-view Composite dataset.\n\ncollection_id = \"C1576365803-LARC_ASDC\"  # (~9 to 12 minutes to process) EPIC-view satellite composites for DSCOVR, Version 1\n\nrequest = Request(\n    collection=Collection(id=collection_id),\n    shape=sf_geojson,\n    temporal={\n        'start': dt.datetime(2016, 2, 24, 12),\n        'stop': dt.datetime(2016, 2, 24, 23)   \n    },\n    variables=[\"general/relative_time\", \"general/precipitable_water\", \"clear_sky/skin_temperature\"]\n)\nrequest.is_valid()\n\nTrue\n\n\n\njob_id = harmony_client.submit(request)\njob_id\n\n'696ab111-4e7b-4aa9-bbff-d91c60533cc6'\n\n\n\nharmony_client.status(job_id)\n\n{'status': 'running',\n 'message': 'The job is being processed',\n 'progress': 0,\n 'created_at': datetime.datetime(2023, 12, 7, 20, 41, 23, 881000, tzinfo=tzlocal()),\n 'updated_at': datetime.datetime(2023, 12, 7, 20, 41, 23, 881000, tzinfo=tzlocal()),\n 'created_at_local': '2023-12-07T20:41:23+00:00',\n 'updated_at_local': '2023-12-07T20:41:23+00:00',\n 'request': 'https://harmony.earthdata.nasa.gov/C1576365803-LARC_ASDC/ogc-api-coverages/1.0.0/collections/general%2Frelative_time,general%2Fprecipitable_water,clear_sky%2Fskin_temperature/coverage/rangeset?forceAsync=true&subset=time(%222016-02-24T12%3A00%3A00%22%3A%222016-02-24T23%3A00%3A00%22)',\n 'num_input_granules': 4,\n 'data_expiration': datetime.datetime(2024, 1, 6, 20, 41, 23, 881000, tzinfo=tzlocal()),\n 'data_expiration_local': '2024-01-06T20:41:23+00:00'}\n\n\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\n [ Processing: 100% ] |###################################################| [|]\n\n\nWhile this processes, we can discuss the harmony job in some more detail. First, note that this request is identical to what can be achieved through NASA’s Earthdata Search interface, such as this URL: https://search.earthdata.nasa.gov/search/granules?p=C1576365803-LARC_ASDC!C1576365803-LARC_ASDC&pg[1][a]=1576368528!1576368575!LARC_ASDC&pg[1][v]=t&pg[1][gsk]=-start_date&pg[1][m]=harmony0&pg[1][of]=application/x-netcdf4&pg[1][ets]=t&pg[1][ess]=t&q=C1576365803-LARC_ASDC&sb[0]=-123.99609%2C37.19991%2C-120.44531%2C38.78263&qt=2016-02-24T12%3A00%3A00.000Z%2C2016-02-24T23%3A00%3A00.000Z&tl=1702228562!3!!&lat=37.8270894268111&long=-130.67578125&zoom=4\n(Futher information and examples can be found in the harmony-py repository, such as this introductory notebook.)\n\n\nRequest Parameters\nIn addition to the above request parameters, other advanced parameters may be of interest. Note that many reformatting or advanced projection options may not be available for your requested dataset. See the documentation for details on how to construct these parameters.\n\ncrs: Reproject the output coverage to the given CRS. Recognizes CRS types that can be inferred by gdal, including EPSG codes, Proj4 strings, and OGC URLs (http://www.opengis.net/def/crs/%E2%80%A6)\ninterpolation: specify the interpolation method used during reprojection and scaling\nscale_extent: scale the resulting coverage either among one axis to a given extent\nscale_size: scale the resulting coverage either among one axis to a given size\ngranule_id: The CMR Granule ID for the granule (file) which should be retrieved\nwidth: number of columns to return in the output coverage\nheight: number of rows to return in the output coverage\nformat: the output mime type to return\nmax_results: limits the number of input files processed in the request\n\n\n\nHarmony Client\nThere are four options for providing your Earthdata Login token or username and password when creating a Harmony Client:\n\nProvide EDL token using environment variable, e.g.:\n\n\n$ export EDL_TOKEN='my_eld_token'\n\n\nProvide your username and password directly when creating the client:\n\n\nharmony_client = Client(auth=('captainmarvel', 'marve10u5'))\n\n\nSet your credentials using environment variables:\n\nYou can either export these directly:\n\n$ export EDL_USERNAME='captainmarvel'\n\n\n$ export EDL_PASSWORD='marve10u5'\n\nOr by storing them in a .env file, which operates in a similar fashion to .netrc. You will need to store the file in your current working directory and it must be named .env with the following format:\n\nEDL_USERNAME=myusername\n\n\nEDL_PASSWORD=mypass\n\n\nUse a .netrc file:\n\n\n``` machine urs.earthdata.nasa.gov login captainmarvel password marve10u5\n\n\n::: {.cell tags='[]' execution_count=25}\n``` {.python .cell-code}\nharmony_client.status(job_id)\n\n{'status': 'successful',\n 'message': 'The job has completed successfully',\n 'progress': 100,\n 'created_at': datetime.datetime(2023, 12, 7, 20, 41, 23, 881000, tzinfo=tzlocal()),\n 'updated_at': datetime.datetime(2023, 12, 7, 20, 47, 30, 711000, tzinfo=tzlocal()),\n 'created_at_local': '2023-12-07T20:41:23+00:00',\n 'updated_at_local': '2023-12-07T20:47:30+00:00',\n 'request': 'https://harmony.earthdata.nasa.gov/C1576365803-LARC_ASDC/ogc-api-coverages/1.0.0/collections/general%2Frelative_time,general%2Fprecipitable_water,clear_sky%2Fskin_temperature/coverage/rangeset?forceAsync=true&subset=time(%222016-02-24T12%3A00%3A00%22%3A%222016-02-24T23%3A00%3A00%22)',\n 'num_input_granules': 4,\n 'data_expiration': datetime.datetime(2024, 1, 6, 20, 41, 23, 881000, tzinfo=tzlocal()),\n 'data_expiration_local': '2024-01-06T20:41:23+00:00'}\n\n:::\n\nresults = harmony_client.result_json(job_id, show_progress=True)\n# results\n\n [ Processing: 100% ] |###################################################| [|]\n\n\n\nprint('\\nDownloading results:')\nfutures = harmony_client.download_all(job_id)\n\nfilenames = []\nfor f in futures:\n    fn = f.result()\n    filenames.append(fn)\n\nprint('\\nDone downloading.')\n\n\nDownloading results:\nDSCOVR_EPIC_L2_COMPOSITE_01_20160224T163900Z_subsetted.nc4\nDSCOVR_EPIC_L2_COMPOSITE_01_20160224T201500Z_subsetted.nc4\nDSCOVR_EPIC_L2_COMPOSITE_01_20160224T182700Z_subsetted.nc4\nDSCOVR_EPIC_L2_COMPOSITE_01_20160225T004500Z_subsetted.nc4\n\nDone downloading.\n\n\n\nnew_ds_general = xr.open_dataset(filenames[0], group=\"general\", decode_times=False)\nnew_ds_general\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:             (Lines: 25, Pixels: 37)\nCoordinates:\n    lat                 (Lines, Pixels) float32 ...\n    lon                 (Lines, Pixels) float32 ...\nDimensions without coordinates: Lines, Pixels\nData variables:\n    relative_time       (Lines, Pixels) float64 ...\n    precipitable_water  (Lines, Pixels) float32 ...xarray.DatasetDimensions:Lines: 25Pixels: 37Coordinates: (2)lat(Lines, Pixels)float32...valid_range :[-90.  90.]long_name :latitudeunits :degrees_north[925 values with dtype=float32]lon(Lines, Pixels)float32...valid_range :[-180.  180.]long_name :longitudeunits :degrees_east[925 values with dtype=float32]Data variables: (2)relative_time(Lines, Pixels)float64...long_name :relative_timevalid_range :[-32767  32767]units :seconds since 2016-02-24 17:03:16calendar :proleptic_gregorian[925 values with dtype=float64]precipitable_water(Lines, Pixels)float32...long_name :MOA Precipitable Waterunits :cmvalid_range :[    0 50000][925 values with dtype=float32]Indexes: (0)Attributes: (0)\n\n\n\n\n\nPlotting\n\nprecip_attrs = new_ds_general['precipitable_water'].attrs\nprecip_attrs\n\n{'long_name': 'MOA Precipitable Water',\n 'units': 'cm',\n 'valid_range': array([    0, 50000], dtype=uint16)}\n\n\n\n# Create figure\nproj = ccrs.PlateCarree()\nfig, ax = plt.subplots(figsize=(6, 4), facecolor=\"w\", subplot_kw=dict(projection=proj))\n\nax_handle = ax.contourf(new_ds_general['lon'], new_ds_general['lat'], new_ds_general['precipitable_water'], cmap=\"turbo\")\nplt.colorbar(\n    ax_handle, ax=ax, \n    label=f\"{precip_attrs['long_name']} ({precip_attrs['units']})\"\n)\n\nax.add_feature(cfeature.STATES)\nax.set_extent([-125, -113.0, 31.0, 43.0], crs=proj)\n\nax.set_xticks([-125, -120, -115, -110], crs=proj)\nax.set_yticks([32, 34, 36, 38, 40, 42], crs=proj)\nlon_formatter = LongitudeFormatter(number_format='.1f',\n                                   degree_symbol='',\n                                   dateline_direction_label=True)\nlat_formatter = LatitudeFormatter(number_format='.1f',\n                                  degree_symbol='')\nax.xaxis.set_major_formatter(lon_formatter)\nax.yaxis.set_major_formatter(lat_formatter)\n\nplt.show()\n\n\n\n\nA final note about harmony-py: As more transformation services are added and configured to work with existing and new NASA data collections, the capabilities you will be able to harness with harmony-py will also grow!"
  },
  {
    "objectID": "tutorials/Earthdata_Subset_and_Plot.html#appendix-1---snow-cover-from-modisterra-daily-level3",
    "href": "tutorials/Earthdata_Subset_and_Plot.html#appendix-1---snow-cover-from-modisterra-daily-level3",
    "title": "Part 2 of the In-cloud Science Workflow: Data subsetting and plotting with earthaccess, xarray, and harmony",
    "section": "Appendix 1 - Snow Cover from MODIS/Terra, Daily Level3",
    "text": "Appendix 1 - Snow Cover from MODIS/Terra, Daily Level3\n\nDataset\nWe will use MODIS/Terra Snow Cover Daily L3 Global 0.05Deg CMG. The Moderate Resolution Imaging Spectroradiometer (MODIS) global Level-3 (L3) data set provides the percentage of snow-covered land and cloud-covered land observed daily, within 0.05° (approx. 5 km) MODIS Climate Modeling Grid (CMG) cells.\nThe dataset and algorithm is described in the data user guide and the Product Specific Document.\nPlease cite the dataset as: &gt; Hall, D. K. and G. A. Riggs. (2021). MODIS/Terra Snow Cover Daily L3 Global 0.05Deg CMG, Version 61. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/MODIS/MOD10C1.061.\nUsing the dataset DOI, we will use the earthaccess module to search for dataset granules from February 24, 2023, and March 2, 2023.\n\ndoi = '10.5067/MODIS/MOD10C1.061' # MODIS Terra Snowcover\n\n# search granules from Feb 15, 2023\ndate1 = \"2023-02-15\"\ngranules1 = earthaccess.search_data(\n    count=-1, # needed to retrieve all granules\n    doi=doi,\n    temporal=(date1, date1)\n)\n# search granules from March 02, 2023\ndate2 = \"2023-03-02\"\ngranules2 = earthaccess.search_data(\n    count=-1, # needed to retrieve all granules\n    doi=doi,\n    temporal=(date2, date2)\n)\n\nGranules found: 1\nGranules found: 1\n\n\nLet’s download the granules to the local environment. This is needed as direct access to HDF4 files that MODIS Collection 6.1 comes as is currently not supported. The earthaccess module manages the authentication that is required for accessing data.\n\nearthaccess.download(granules1, local_path='.')\nearthaccess.download(granules2, local_path='.')\n\n Getting 1 granules, approx download size: 0.0 GB\n Getting 1 granules, approx download size: 0.0 GB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n['MOD10C1.A2023061.061.2023070101022.hdf']\n\n\n\n\nSubsetting\nLet’s open the downloaded granules into a rioxarray. The variable Day_CMG_Snow_Cover provides daily percent snow in 5km grids. The variable Snow_Spatial_QA provides quality indicator for each grid: 0=best, 1=good, 2=ok, 3=poor, 4=other, 237=inland water, 239=ocean, 250=cloud obscured water 252=Antarctica mask, 253=not mapped, 254=no retrieval, and 255=fill. We will only use the grids with 0, 1, and 2 quality flags.\n\n# open granule from Feb 15, 2023\ng_1 = Path(Path(granules1[0].data_links()[0]).name)\nif g_1.is_file():\n    with rxr.open_rasterio(g_1) as modis:\n        print(modis)\n        snow_cover1 = modis['Day_CMG_Snow_Cover'][:]\n        snow_cover_qa1 = modis['Snow_Spatial_QA'][:]\n\n# open granules from March 02, 2023\ng_2 = Path(Path(granules2[0].data_links()[0]).name)\nif g_2.is_file():\n    with rxr.open_rasterio(g_2) as modis:\n        snow_cover2 = modis['Day_CMG_Snow_Cover'][:]\n        snow_cover_qa2 = modis['Snow_Spatial_QA'][:]\n\n# Spatially subset and keep only good quality cells\nsnow_cover_good1 = (\n    snow_cover1\n    .sel(x=slice(-125, -113), y=slice(43, 31))\n    .where((snow_cover_qa1 &gt;= 0) & (snow_cover_qa1 &lt;= 2))\n)\nsnow_cover_good2 = (\n    snow_cover2\n    .sel(x=slice(-125, -113), y=slice(43, 31))\n    .where((snow_cover_qa2 &gt;= 0) & (snow_cover_qa2 &lt;= 2))\n)\n\n&lt;xarray.Dataset&gt;\nDimensions:                 (band: 1, x: 7200, y: 3600)\nCoordinates:\n  * band                    (band) int64 1\n  * x                       (x) float64 -180.0 -179.9 -179.9 ... 179.9 180.0\n  * y                       (y) float64 89.97 89.92 89.88 ... -89.93 -89.98\n    spatial_ref             int64 0\nData variables:\n    Day_CMG_Snow_Cover      (band, y, x) uint8 ...\n    Day_CMG_Clear_Index     (band, y, x) uint8 ...\n    Day_CMG_Cloud_Obscured  (band, y, x) uint8 ...\n    Snow_Spatial_QA         (band, y, x) uint8 ...\nAttributes: (50)\n\n\n\n\nPlotting\n\n# create the plot\nproj = ccrs.PlateCarree()\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4), dpi=130, facecolor=\"w\", subplot_kw=dict(projection=proj))\n\nsnowax1 = ax1.pcolormesh(snow_cover_good1.x.values, snow_cover_good1.y.values, snow_cover_good1.values[0], vmax=100, cmap='Blues')\nplt.colorbar(snowax1, ax=ax1, label=\"snow cover (%)\")\nax1.add_feature(cfeature.STATES)\nax1.set_title(f'Snow Cover {date1}')\n\nsnowax2 = ax2.pcolormesh(snow_cover_good2.x.values, snow_cover_good2.y.values, snow_cover_good2.values[0], vmax=100, cmap='Blues')\nplt.colorbar(snowax2, ax=ax2, label=\"snow cover (%)\")\nax2.add_feature(cfeature.STATES)\nax2.set_title(f'Snow Cover {date2}')\nplt.show()"
  },
  {
    "objectID": "tutorials/Earthdata_Subset_and_Plot.html#appendix-2---snow-mass-from-smap-3-hourly-level-4",
    "href": "tutorials/Earthdata_Subset_and_Plot.html#appendix-2---snow-mass-from-smap-3-hourly-level-4",
    "title": "Part 2 of the In-cloud Science Workflow: Data subsetting and plotting with earthaccess, xarray, and harmony",
    "section": "Appendix 2 - Snow Mass from SMAP, 3-hourly Level 4",
    "text": "Appendix 2 - Snow Mass from SMAP, 3-hourly Level 4\n\nDataset\nThe Soil Moisture Active Passive (SMAP) L4 Global 3-hourly 9 km EASE-Grid Surface and Root Zone Soil Moisture Geophysical Data (SPL4SMGP) provides a model-derived global 3-hr time average of snow mass in kg/m2. SMAP L-band brightness temperature data from descending and ascending half-orbit satellite passes (approximately 6:00 a.m. and 6:00 p.m. local solar time, respectively) are assimilated into a land surface model that is gridded using an Earth-fixed, global cylindrical 9 km Equal-Area Scalable Earth Grid, Version 2.0 (EASE-Grid 2.0) projection. Snow mass estimates are based on a snow model component of the NASA Catchment Land Surface Model.\nThe dataset and algorithm are described in the data user guide and the Product Specific Document.\nPlease cite the dataset as: &gt; Reichle, R., G. De Lannoy, R. D. Koster, W. T. Crow, J. S. Kimball, Q. Liu, and M. Bechtold. (2022). SMAP L4 Global 3-hourly 9 km EASE-Grid Surface and Root Zone Soil Moisture Geophysical Data, Version 7. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/EVKPQZ4AFC4D.\n\n# SMAP SPL4SMGP\ndoi = '10.5067/EVKPQZ4AFC4D'\n\n# search granules from Feb 15, 2023\ndate1 = \"2023-02-15\"\ngranules1 = earthaccess.search_data(\n    count=-1, # needed to retrieve all granules\n    doi=doi,\n    temporal=(date1, date1)\n)\n\n# search granules from March 02, 2023\ndate2 = \"2023-03-02\"\ngranules2 = earthaccess.search_data(\n    count=-1, # needed to retrieve all granules\n    doi=doi,\n    temporal=(date2, date2)\n)\n\nGranules found: 2\nGranules found: 2\n\n\n\n# granules from Feb 15, 2023\nfh1 = earthaccess.open(granules1)\n# open geophysical_data group\nds1 = xr.open_dataset(fh1[0], phony_dims='access', group='Geophysical_Data')\n# get location\nds_loc1 = xr.open_dataset(fh1[0], phony_dims='access')\n\n Opening 2 granules, approx size: 0.27 GB\n\n\n\n\n\n\n\n\n\n\n\n\n# granules from March 02, 2023\nfh2 = earthaccess.open(granules2)\n# open geophysical_data group\nds2 = xr.open_dataset(fh2[0], phony_dims='access', group='Geophysical_Data')\n# get location\nds_loc2 = xr.open_dataset(fh2[0], phony_dims='access')\n\n Opening 2 granules, approx size: 0.27 GB\n\n\n\n\n\n\n\n\n\n\n\nThe “snow_mass” variable is within the “Geophysical_Data” group. It provides the average snow mass (or snow water equivalent) over a land fraction of the grid cell, excluding areas of open water and permanent ice. Let’s print the attributes of the snow_mass variable.\n\nds1.variables['snow_mass'].attrs\n\n{'DIMENSION_LABELS': [b'y', b'x'],\n 'coordinates': '/cell_lat /cell_lon',\n 'fmissing_value': -9999.0,\n 'grid_mapping': 'EASE2_global_projection',\n 'long_name': 'Average snow mass (or snow water equivalent) over land fraction of grid cell (excluding areas of open water and permanent ice)',\n 'units': 'kg m-2',\n 'valid_max': 10000.0,\n 'valid_min': 0.0}\n\n\n\n# create the plot\nproj = ccrs.Projection(\"EPSG:6933\") # EASEGRID 2\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4), dpi=130, facecolor=\"w\", subplot_kw=dict(projection=proj))\n\nca_bounds = [-12060785, -10902950, 3769089, 4995383]\n\nsnow_mass1 = ds1.snow_mass.where(ds1.snow_mass&gt;9.4)\nsnowax1 = ax1.pcolormesh(ds_loc1.x, ds_loc1.y, snow_mass1, vmax=200, cmap='Blues')\nplt.colorbar(snowax1, ax=ax1, label=\"snow mass (kg/m2)\")\nax1.add_feature(cfeature.STATES)\nax1.set_extent(ca_bounds, crs=proj)\nax1.set_title(f'Snow Mass {date1}')\n\nsnow_mass2 = ds2.snow_mass.where(ds2.snow_mass&gt;9.4)\nsnowax2 = ax2.pcolormesh(ds_loc2.x, ds_loc2.y, snow_mass2, vmax=200, cmap='Blues')\nplt.colorbar(snowax2, ax=ax2, label=\"snow mass (kg/m2)\")\nax2.add_feature(cfeature.STATES)\nax2.set_extent(ca_bounds, crs=proj)\nax2.set_title(f'Snow Mass {date2}')\nplt.show()\n\n\n\n\n\n\nNow we will remove the saved files from our workspace, to keep it clean for future coding!\n\nfrom glob import glob\n\nfor f in glob(\"DSCOVR_EPIC_L2*.nc4\"):\n    Path(f).unlink()\n    \nfor f in glob(\"MOD10C1.*.hdf\"):\n    Path(f).unlink()"
  },
  {
    "objectID": "tutorials/Earthdata_Subset_and_Plot.html#additional-resources",
    "href": "tutorials/Earthdata_Subset_and_Plot.html#additional-resources",
    "title": "Part 2 of the In-cloud Science Workflow: Data subsetting and plotting with earthaccess, xarray, and harmony",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nTutorials\nThis clinic was based off of several notebook tutorials including those presented during past workshop events, along with other materials co-created by the NASA Openscapes mentors: * 2021 Earthdata Cloud Hackathon * 2021 AGU Workshop * Accessing and working with ICESat-2 data in the cloud * Analyzing Sea Level Rise Using Earth Data in the Cloud\n\n\nCloud services\nThe examples used in the clinic provide an abbreviated and simplified workflow to explore access and subsetting options available through the Earthdata Cloud. There are several other options that can be used to interact with data in the Earthdata Cloud including:\n\nOPeNDAP\n\nHyrax provides direct access to subsetting of NASA data using Python or your favorite analysis tool\nTutorial highlighting OPeNDAP usage: https://nasa-openscapes.github.io/earthdata-cloud-cookbook/how-tos/working-locally/Earthdata_Cloud__Data_Access_OPeNDAP_Example.html\n\nZarr-EOSDIS-Store\n\nThe zarr-eosdis-store library allows NASA EOSDIS Collections to be accessed efficiently by the Zarr Python library, provided they have a sidecar DMR++ metadata file generated.\nTutorial highlighting this library’s usage: https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/09_Zarr_Access.html\n\n\n\n\nSupport\n\nEarthdata Forum\n\nUser Services and community support for all things NASA Earthdata, including Earthdata Cloud\n\nEarthdata Webinar series\n\nWebinars from DAACs and other groups across EOSDIS including guidance on working with Earthdata Cloud\nSee the Earthdata YouTube channel for more videos\n\n\nEND of Notebook."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The Enabling Analysis in the Cloud Using NASA Earth Science Data will take place on Sunday, December 10 from 13:00-16:30.\nZoom links will be shared directly with this group via a (calendar) meeting invite.\nNote, hands-on exercises will be executed from a Jupyter Lab instance in 2i2c. Click here to deploy the instance and simultaneously clone this GitHub repository to follow along with the tutorials. Please pass along your GitHub Username to get access."
  },
  {
    "objectID": "schedule.html#workshop-schedule",
    "href": "schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\nDecember 10, 2023\n\n\n\nTime, PST (UTC-7)\nEvent\nLeads/Instructors\n\n\n\n\n1:00 pm\nWelcome / Open Science Mindset\nCassie Nickles (PO.DAAC)\n\n\n1:20 pm\nIntroduction to NASA Earthdata Cloud\nMichele Thornton (ORNL DAAC)\n\n\n1:40 pm\nOrientation and setup in JupyterHub in the Cloud\nMichele Thornton (ORNL DAAC)\n\n\n2:00 pm\nTutorial: In-cloud Science Workflow: Search & Discovery\nAaron Friesz (LP DAAC)\n\n\n2:30 pm\nBreak\n\n\n\n2:45 pm\nTutorial: In-cloud Science Workflow: Access, Subset & Plot\nDanny Kaufman (ASDC DAAC)\n\n\n3:45 pm\nQ & A\nAll NASA Openscapes Mentors\n\n\n4:15 pm\nClosing Survey\nCassie Nickles (PO.DAAC)"
  },
  {
    "objectID": "schedule.html#closing---close-out-your-jupyter-hub",
    "href": "schedule.html#closing---close-out-your-jupyter-hub",
    "title": "Schedule",
    "section": "Closing - Close out your Jupyter Hub!",
    "text": "Closing - Close out your Jupyter Hub!\n\nClose out your JupyterHub instance if you are finished for the day, following these instructions.\nYou will continue to have access to the 2i2c JupyterHub in AWS for two weeks following this Workshop. You may use that time on your own to continue work and learn more about migrating data access routines and science workflows to the cloud. This cloud compute environment is supported by the NASA Openscapes project.\n\n\nThank you!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "section": "",
    "text": "Welcome to the 2023 NASA Openscapes AGU Workshop hosted by NASA Openscapes!\nThe workshop will take place in person and virtually at AGU on Sunday, December 10, 2023 from 1pm-4:30pm PST (UTC-7)."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "section": "",
    "text": "Welcome to the 2023 NASA Openscapes AGU Workshop hosted by NASA Openscapes!\nThe workshop will take place in person and virtually at AGU on Sunday, December 10, 2023 from 1pm-4:30pm PST (UTC-7)."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "section": "About",
    "text": "About\n\nWorkshop Goals\n\nProvide an inclusive place to learn about and experience working with NASA Earthdata Cloud;\nProvide strategies and best practices for finding and accessing NASA Earthdata Cloud data to help promote interdisciplinary research. Specifically, participants will learn how to access data from AWS S3 buckets and have a better understanding of the Python ecosystem used to analyze the data depending on size and data formats;\nBetter understand the needs of Earthdata data scientists by providing a pre/post survey and engaging in discussions with workshop participants;\nIdentify and practice using popular open source tools and utilities.\n\n\n\nWorkshop Description\nEarth science data, including NASA Earthdata, is increasingly more available from the cloud. By leveraging cloud compute resources, open science principles, and technologies that foster transparency and reproducibility, science can be done at rates and scales that are not achievable by using personal/local machines. Building modern data skills in a friendly environment is crucial for accelerating science and reducing barriers to inclusive scientific research. In this workshop, NASA Openscapes Mentors from NASA’s Earth Observing System Data and Information System (EOSDIS) DAACs (data centers) will teach the foundations of an open science mindset and apply these concepts to work in the cloud with NASA Earthdata. Participants will take part in hands-on tutorials using a JupyterHub managed by 2i2c in AWS. Participants will leave with a better understanding of how to leverage data and services from NASA Earthdata Cloud within their work across a variety of disciplines and data types, as well as how to apply the concepts of open science as a daily practice. The workshop will encourage discussion and reflection on how Earth science is evolving. Tutorials will be taught in Python. The target audience is anyone interested in using NASA Earth Science data within the AWS cloud. Previous experience in the AWS cloud is not necessary. Experience using Python is recommended but not required.\n\n\nLearning Outcomes\nUpon completion of the proposed workshop, participants will leave with a better understanding of how NASA Earthdata Cloud data and services can best be leveraged and integrated within their work across a variety of disciplines and data types, while being exposed to open science practices and workflows in Python. Participants will be engaged on two different levels: (1) through an open science mindset, and (2) through science use cases that demonstrate capabilities for efficient data discovery, access, and use within the cloud. These learning objectives will encourage discussion and reflection on how the Earth science, data science, and informatics communities are evolving, and how this evolution presents challenges and opportunities in scientific research."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis Workshop has been developed as a cross-DAAC collaboration by the NASA-Openscapes Team. Learn more at nasa-openscapes.github.io."
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html",
    "title": "Data discovery with earthaccess",
    "section": "",
    "text": "In this example we will use the earthaccess library to search for data collections from NASA Earthdata. earthaccess is a Python library that simplifies data discovery and access to NASA Earth science data by providing an abstraction layer for NASA’s Common Metadata Repository (CMR) API Search API. The library makes searching for data more approachable by using a simpler notation instead of low level HTTP queries. earthaccess takes the trouble out of Earthdata Login authentication, makes search easier, and provides a stream-line way to download or stream search results into an xarray object.\nFor more on earthaccess visit the earthaccess GitHub page and/or the earthaccess documentation site. Be aware that earthaccess is under active development."
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#summary",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#summary",
    "title": "Data discovery with earthaccess",
    "section": "",
    "text": "In this example we will use the earthaccess library to search for data collections from NASA Earthdata. earthaccess is a Python library that simplifies data discovery and access to NASA Earth science data by providing an abstraction layer for NASA’s Common Metadata Repository (CMR) API Search API. The library makes searching for data more approachable by using a simpler notation instead of low level HTTP queries. earthaccess takes the trouble out of Earthdata Login authentication, makes search easier, and provides a stream-line way to download or stream search results into an xarray object.\nFor more on earthaccess visit the earthaccess GitHub page and/or the earthaccess documentation site. Be aware that earthaccess is under active development."
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#prerequisites",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#prerequisites",
    "title": "Data discovery with earthaccess",
    "section": "Prerequisites",
    "text": "Prerequisites\nAn Earthdata Login account is required to access data from NASA Earthdata. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up."
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#learning-objectives",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#learning-objectives",
    "title": "Data discovery with earthaccess",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nHow to authenticate with earthaccess\nHow to use earthaccess to search for data using spatial and temporal filters\nHow to explore and work with search results"
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#get-started",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#get-started",
    "title": "Data discovery with earthaccess",
    "section": "Get Started",
    "text": "Get Started\n\nImport Required Packages\n\nimport earthaccess \nfrom pprint import pprint\nimport xarray as xr\nimport geopandas as gpd\n\n\n\nAuthentication for NASA Earthdata\nWe will start by authenticating using our Earthdata Login credentials. Authentication is not necessarily needed to search for publicaly available data collections in Earthdata, but is always need to download or access data from the NASA Earthdata archives. We can use login method from the earthaccess library here. This will create a authenticated session using our Earthdata Login credential. Our credentials can be passed along via environmental variables or by a .netrc file save in the home/user profile directory. If your credentials are not available in either location, we will be prompt to input our credentials and a .netrc will be created and saved for us.\n\nauth = earthaccess.login()\n# are we authenticated?\nif not auth.authenticated:\n    # ask for credentials and persist them in a .netrc file\n    auth.login(strategy=\"interactive\", persist=True)\n\nEARTHDATA_USERNAME and EARTHDATA_PASSWORD are not set in the current environment, try setting them or use a different strategy (netrc, interactive)\nYou're now authenticated with NASA Earthdata Login\nUsing token with expiration date: 02/02/2024\nUsing .netrc file for EDL\n\n\n\n\nSearch for data\nThere are multiple keywords we can use to discovery data from collections. The table below contains the short_name, concept_id, and doi for some collections we are interested in for other exercises. Each of these can be used to search for data or information related to the collection we are interested in.\n\n\n\nShortname\nCollection Concept ID\nDOI\n\n\n\n\nGPM_3IMERGDF\nC2723754864-GES_DISC\n10.5067/GPM/IMERGDF/DAY/07\n\n\nMOD10C1\nC1646609808-NSIDC_ECS\n10.5067/MODIS/MOD10C1.061\n\n\nSPL4SMGP\nC2531308461-NSIDC_ECS\n10.5067/EVKPQZ4AFC4D\n\n\nSPL4SMAU\nC2537927247-NSIDC_ECS\n10.5067/LWJ6TF5SZRG3\n\n\n\nBut wait…You may be asking “how can we find the shortname, concept_id, and doi for collections not in the table above?”. Let’s take a quick detour.\nhttps://search.earthdata.nasa.gov/search?q=GPM_3IMERGDF\n\nSearch by collection\n\ncollection_id = 'C2723754864-GES_DISC'\n\n\nresults = earthaccess.search_data(\n    concept_id = collection_id,\n    cloud_hosted = True,\n    count = 10    # Restricting to 10 records returned\n)\n\nGranules found: 8400\n\n\nIn this example we used the concept_id parameter to search from our desired collection. However, there are multiple ways to specify the collection(s) we are interested in. Alternative parameters include:\n\ndoi - request collection by digital object identifier (e.g., doi = ‘10.5067/GPM/IMERGDF/DAY/07’)\n\nshort_name - request collection by CMR shortname (e.g., short_name = ‘GPM_3IMERGDF’)\n\nNOTE: Each Earthdata collect has a unique concept_id and doi. This is not the case with short_name. A shortname can be associated with multiple versions of a collection. If multiple versions of a collection are publicaly available, using the short_name parameter with return all versions available. It is advised to use the version parameter in conjunction with the short_name parameter with searching.\nWe can refine our search by passing more parameters that describe the spatiotemporal domain of our use case. Here, we use the temporal parameter to request a date range and the bounding_box parameter to request granules that intersect with a bounding box.\nFor our bounding box, we are going to read in a GeoJSON file containing a single feature and extract the coordinate pairs for the southeast corner and the northwest corner (or lowerleft and upperright corners) of the bounding box around the feature.\n\ninGeojson = gpd.read_file('../../2023-Cloud-Workshop-AGU/data/sf_to_sierranvmt.geojson')\n\n\nxmin, ymin, xmax, ymax = inGeojson.total_bounds\n\nWe will assign our start date and end date to a variable named date_range and we’ll assign the southeast and the northwest corner coordinates to a variable named bbox to be passed to our earthaccess search request.\n\ndate_range = (\"2022-11-19\", \"2023-04-06\")\n#bbox = (-127.0761, 31.6444, -113.9039, 42.6310)\nbbox = (xmin, ymin, xmax, ymax)\n\n\nresults = earthaccess.search_data(\n    concept_id = collection_id,\n    cloud_hosted = True,\n    temporal = date_range,\n    bounding_box = bbox,\n)\n\nGranules found: 139\n\n\n\nThe short_name and concept_id search parameters can be used to request one or multiple collections per request, but the doi parameter can only request a single collection.\n&gt; concept_ids = [‘C2723754864-GES_DISC’, ‘C1646609808-NSIDC_ECS’]\n\nUse the cloud_hosted search parameter only to search for data assets available from NASA’s Earthdata Cloud.\nThere are even more search parameters that can be passed to help refine our search, however those parameters do have to be populated in the CMR record to be leveraged. A non exhaustive list of examples are below:\n\nday_night_flag = 'day'\n\ncloud_cover = (0, 10)\n\n\n\n# col_ids = ['C2723754864-GES_DISC', 'C1646609808-NSIDC_ECS', 'C2531308461-NSIDC_ECS', 'C2537927247-NSIDC_ECS']    # Specify a list of collections to pass to the search\n\n# results = earthaccess.search_data(\n#     concept_id = col_ids,\n#     #cloud_hosted = True,\n#     temporal = date_range,\n#     bounding_box = bbox,\n# )\n\n\n\n\nWorking with earthaccess returns\nearthaccess provides several convenience methods to help streamline processes that historically have be painful when done using traditional methods. Following the search for data, you’ll likely take one of two pathways with those results. You may choose to download the assets that have been returned to you or you may choose to continue working with the search results within the Python environment.\n\nDownload earthaccess results\nIn some cases you may want to download your assets. earthaccess makes downloading the data from the search results very easy using the earthaccess.download() function.\n\ndownloaded_files = earthaccess.download(\n    results[0:9],\n    local_path='../../2023-Cloud-Workshop-AGU/data',\n)\n\n Getting 9 granules, approx download size: 0.25 GB\n\n\n\n\n\n\n\n\n\n\n\nearthaccess did a lot of heavy lifting for us. It identified the downloadable links, passed our Earthdata Login credentials, and save off the file with the proper name. Amazing right!?\nWe’re going to remove those files to keep our space clean.\n\n!rm ../../2023-Cloud-Workshop-AGU/data/*.nc4\n\n\n\nExplore earthaccess search response\n\nprint(f'The results variable is a {type(results)} of {type(results[0])}')\n\nThe results variable is a &lt;class 'list'&gt; of &lt;class 'earthaccess.results.DataGranule'&gt;\n\n\n\nlen(results)\n\n139\n\n\nWe can explore the first item (earthaccess.results.DataGranule) in our list.\n\nitem = results[0]\ntype(item)\n\nearthaccess.results.DataGranule\n\n\nEach item contains three keys that can be used to explore the item\n\nitem.keys()\n\ndict_keys(['meta', 'umm', 'size'])\n\n\n\nitem['umm']\n\n{'RelatedUrls': [{'URL': 'https://data.gesdisc.earthdata.nasa.gov/data/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4',\n   'Type': 'GET DATA',\n   'Description': 'Download 3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4'},\n  {'URL': 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4',\n   'Type': 'GET DATA VIA DIRECT ACCESS',\n   'Description': 'This link provides direct download access via S3 to the granule'},\n  {'URL': 'https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4',\n   'Type': 'USE SERVICE API',\n   'Subtype': 'OPENDAP DATA',\n   'Description': 'The OPENDAP location for the granule.',\n   'MimeType': 'application/x-netcdf-4'},\n  {'URL': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n   'Type': 'VIEW RELATED INFORMATION',\n   'Description': 'api endpoint to retrieve temporary credentials valid for same-region direct s3 access'}],\n 'SpatialExtent': {'HorizontalSpatialDomain': {'Geometry': {'BoundingRectangles': [{'WestBoundingCoordinate': -180.0,\n      'EastBoundingCoordinate': 180.0,\n      'NorthBoundingCoordinate': 90.0,\n      'SouthBoundingCoordinate': -90.0}]}}},\n 'ProviderDates': [{'Date': '2023-08-25T14:06:33.000Z', 'Type': 'Insert'},\n  {'Date': '2023-08-25T14:06:33.000Z', 'Type': 'Update'}],\n 'CollectionReference': {'ShortName': 'GPM_3IMERGDF', 'Version': '07'},\n 'DataGranule': {'DayNightFlag': 'Unspecified',\n  'Identifiers': [{'Identifier': '3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4',\n    'IdentifierType': 'ProducerGranuleId'}],\n  'ProductionDateTime': '2023-08-25T14:06:33.000Z',\n  'ArchiveAndDistributionInformation': [{'Name': 'Not provided',\n    'Size': 28.37006378173828,\n    'SizeUnit': 'MB'}]},\n 'TemporalExtent': {'RangeDateTime': {'BeginningDateTime': '2022-11-19T00:00:00.000Z',\n   'EndingDateTime': '2022-11-19T23:59:59.999Z'}},\n 'GranuleUR': 'GPM_3IMERGDF.07:3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4',\n 'MetadataSpecification': {'URL': 'https://cdn.earthdata.nasa.gov/umm/granule/v1.6.5',\n  'Name': 'UMM-G',\n  'Version': '1.6.5'}}\n\n\n\n\nGet data URLs / S3 URIs\nGet links to data. The data_links() method is used to return the URL(s)/data link(s) for the item. By default the method returns the HTTPS URL to download or access the item.\n\nitem.data_links()\n\n['https://data.gesdisc.earthdata.nasa.gov/data/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4']\n\n\nThe data_links() method can also be used to get the s3 URI when we want to perform direct s3 access of the data in the cloud. To get the s3 URI, pass access = 'direct' to the method.\n\nitem.data_links(access='direct')\n\n['s3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4']\n\n\nIf we want to extract all of the data links from our search results and add or save them to a list, we can.\n\ndata_link_list = []\n\nfor granule in results:\n    for asset in granule.data_links(access='direct'):\n        data_link_list.append(asset)\n        \n\n\ndata_link_list[0:9]\n\n['s3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221119-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221120-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221121-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221122-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221123-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221124-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221125-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221126-S000000-E235959.V07.nc4',\n 's3://gesdisc-cumulus-prod-protected/GPM_L3/GPM_3IMERGDF.07/2022/11/3B-DAY.MS.MRG.3IMERG.20221127-S000000-E235959.V07.nc4']\n\n\nWe can pass or read these lists of data links into libraries like xarray, rioxarray, or gdal, but earthaccess has a built-in module for easily reading these data links in.\n\n\nOpen results in xarray\nWe use earthaccess’s open() method to make a connection to and open the files from our search result.\n\nfileset = earthaccess.open(results)\n\n Opening 139 granules, approx size: 3.75 GB\n\n\n\n\n\n\n\n\n\n\n\nThen we pass the fileset object to xarray.\n\nds = xr.open_mfdataset(fileset, chunks = {})\n\nSome really cool things just happened here! Not only were we able to seamlessly stream our earthaccess search results into a xarray dataset using the open_mfdataset() (multi-file) method, but earthaccess determined that we were working from within AWS us-west-2 and accessed the data via direct S3 access! We didn’t have to create a session or a filesystem to authenticate and connect to the data. earthaccess did this for us using the auth object we created at the beginning of this tutorial. If we were not working in AWS us-west-2, earthaccess would “automagically” switch to accessing the data via the HTTPS endpoints and would again handle the authentication for us.\nLet’s take a quick lock at our xarray dataset\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                         (time: 139, lon: 3600, lat: 1800, nv: 2)\nCoordinates:\n  * lon                             (lon) float32 -179.9 -179.9 ... 179.9 179.9\n  * lat                             (lat) float64 -89.95 -89.85 ... 89.85 89.95\n  * time                            (time) datetime64[ns] 2022-11-19 ... 2023...\nDimensions without coordinates: nv\nData variables:\n    precipitation                   (time, lon, lat) float32 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    precipitation_cnt               (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    precipitation_cnt_cond          (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    MWprecipitation                 (time, lon, lat) float32 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    MWprecipitation_cnt             (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    MWprecipitation_cnt_cond        (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    randomError                     (time, lon, lat) float32 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    randomError_cnt                 (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    probabilityLiquidPrecipitation  (time, lon, lat) int8 dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n    time_bnds                       (time, nv) datetime64[ns] dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\nAttributes:\n    BeginDate:       2022-11-19\n    BeginTime:       00:00:00.000Z\n    EndDate:         2022-11-19\n    EndTime:         23:59:59.999Z\n    FileHeader:      StartGranuleDateTime=2022-11-19T00:00:00.000Z;\\nStopGran...\n    InputPointer:    3B-HHR.MS.MRG.3IMERG.20221119-S000000-E002959.0000.V07A....\n    title:           GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 ...\n    DOI:             10.5067/GPM/IMERGDF/DAY/07\n    ProductionTime:  2023-08-25T14:03:25.792Zxarray.DatasetDimensions:time: 139lon: 3600lat: 1800nv: 2Coordinates: (3)lon(lon)float32-179.9 -179.9 ... 179.9 179.9units :degrees_eastlong_name :Longitudearray([-179.95, -179.85, -179.75, ...,  179.75,  179.85,  179.95],\n      dtype=float32)lat(lat)float64-89.95 -89.85 ... 89.85 89.95units :degrees_northlong_name :Latitudearray([-89.95, -89.85, -89.75, ...,  89.75,  89.85,  89.95])time(time)datetime64[ns]2022-11-19 ... 2023-04-06standard_name :timelong_name :timebounds :time_bndsarray(['2022-11-19T00:00:00.000000000', '2022-11-20T00:00:00.000000000',\n       '2022-11-21T00:00:00.000000000', '2022-11-22T00:00:00.000000000',\n       '2022-11-23T00:00:00.000000000', '2022-11-24T00:00:00.000000000',\n       '2022-11-25T00:00:00.000000000', '2022-11-26T00:00:00.000000000',\n       '2022-11-27T00:00:00.000000000', '2022-11-28T00:00:00.000000000',\n       '2022-11-29T00:00:00.000000000', '2022-11-30T00:00:00.000000000',\n       '2022-12-01T00:00:00.000000000', '2022-12-02T00:00:00.000000000',\n       '2022-12-03T00:00:00.000000000', '2022-12-04T00:00:00.000000000',\n       '2022-12-05T00:00:00.000000000', '2022-12-06T00:00:00.000000000',\n       '2022-12-07T00:00:00.000000000', '2022-12-08T00:00:00.000000000',\n       '2022-12-09T00:00:00.000000000', '2022-12-10T00:00:00.000000000',\n       '2022-12-11T00:00:00.000000000', '2022-12-12T00:00:00.000000000',\n       '2022-12-13T00:00:00.000000000', '2022-12-14T00:00:00.000000000',\n       '2022-12-15T00:00:00.000000000', '2022-12-16T00:00:00.000000000',\n       '2022-12-17T00:00:00.000000000', '2022-12-18T00:00:00.000000000',\n       '2022-12-19T00:00:00.000000000', '2022-12-20T00:00:00.000000000',\n       '2022-12-21T00:00:00.000000000', '2022-12-22T00:00:00.000000000',\n       '2022-12-23T00:00:00.000000000', '2022-12-24T00:00:00.000000000',\n       '2022-12-25T00:00:00.000000000', '2022-12-26T00:00:00.000000000',\n       '2022-12-27T00:00:00.000000000', '2022-12-28T00:00:00.000000000',\n       '2022-12-29T00:00:00.000000000', '2022-12-30T00:00:00.000000000',\n       '2022-12-31T00:00:00.000000000', '2023-01-01T00:00:00.000000000',\n       '2023-01-02T00:00:00.000000000', '2023-01-03T00:00:00.000000000',\n       '2023-01-04T00:00:00.000000000', '2023-01-05T00:00:00.000000000',\n       '2023-01-06T00:00:00.000000000', '2023-01-07T00:00:00.000000000',\n       '2023-01-08T00:00:00.000000000', '2023-01-09T00:00:00.000000000',\n       '2023-01-10T00:00:00.000000000', '2023-01-11T00:00:00.000000000',\n       '2023-01-12T00:00:00.000000000', '2023-01-13T00:00:00.000000000',\n       '2023-01-14T00:00:00.000000000', '2023-01-15T00:00:00.000000000',\n       '2023-01-16T00:00:00.000000000', '2023-01-17T00:00:00.000000000',\n       '2023-01-18T00:00:00.000000000', '2023-01-19T00:00:00.000000000',\n       '2023-01-20T00:00:00.000000000', '2023-01-21T00:00:00.000000000',\n       '2023-01-22T00:00:00.000000000', '2023-01-23T00:00:00.000000000',\n       '2023-01-24T00:00:00.000000000', '2023-01-25T00:00:00.000000000',\n       '2023-01-26T00:00:00.000000000', '2023-01-27T00:00:00.000000000',\n       '2023-01-28T00:00:00.000000000', '2023-01-29T00:00:00.000000000',\n       '2023-01-30T00:00:00.000000000', '2023-01-31T00:00:00.000000000',\n       '2023-02-01T00:00:00.000000000', '2023-02-02T00:00:00.000000000',\n       '2023-02-03T00:00:00.000000000', '2023-02-04T00:00:00.000000000',\n       '2023-02-05T00:00:00.000000000', '2023-02-06T00:00:00.000000000',\n       '2023-02-07T00:00:00.000000000', '2023-02-08T00:00:00.000000000',\n       '2023-02-09T00:00:00.000000000', '2023-02-10T00:00:00.000000000',\n       '2023-02-11T00:00:00.000000000', '2023-02-12T00:00:00.000000000',\n       '2023-02-13T00:00:00.000000000', '2023-02-14T00:00:00.000000000',\n       '2023-02-15T00:00:00.000000000', '2023-02-16T00:00:00.000000000',\n       '2023-02-17T00:00:00.000000000', '2023-02-18T00:00:00.000000000',\n       '2023-02-19T00:00:00.000000000', '2023-02-20T00:00:00.000000000',\n       '2023-02-21T00:00:00.000000000', '2023-02-22T00:00:00.000000000',\n       '2023-02-23T00:00:00.000000000', '2023-02-24T00:00:00.000000000',\n       '2023-02-25T00:00:00.000000000', '2023-02-26T00:00:00.000000000',\n       '2023-02-27T00:00:00.000000000', '2023-02-28T00:00:00.000000000',\n       '2023-03-01T00:00:00.000000000', '2023-03-02T00:00:00.000000000',\n       '2023-03-03T00:00:00.000000000', '2023-03-04T00:00:00.000000000',\n       '2023-03-05T00:00:00.000000000', '2023-03-06T00:00:00.000000000',\n       '2023-03-07T00:00:00.000000000', '2023-03-08T00:00:00.000000000',\n       '2023-03-09T00:00:00.000000000', '2023-03-10T00:00:00.000000000',\n       '2023-03-11T00:00:00.000000000', '2023-03-12T00:00:00.000000000',\n       '2023-03-13T00:00:00.000000000', '2023-03-14T00:00:00.000000000',\n       '2023-03-15T00:00:00.000000000', '2023-03-16T00:00:00.000000000',\n       '2023-03-17T00:00:00.000000000', '2023-03-18T00:00:00.000000000',\n       '2023-03-19T00:00:00.000000000', '2023-03-20T00:00:00.000000000',\n       '2023-03-21T00:00:00.000000000', '2023-03-22T00:00:00.000000000',\n       '2023-03-23T00:00:00.000000000', '2023-03-24T00:00:00.000000000',\n       '2023-03-25T00:00:00.000000000', '2023-03-26T00:00:00.000000000',\n       '2023-03-27T00:00:00.000000000', '2023-03-28T00:00:00.000000000',\n       '2023-03-29T00:00:00.000000000', '2023-03-30T00:00:00.000000000',\n       '2023-03-31T00:00:00.000000000', '2023-04-01T00:00:00.000000000',\n       '2023-04-02T00:00:00.000000000', '2023-04-03T00:00:00.000000000',\n       '2023-04-04T00:00:00.000000000', '2023-04-05T00:00:00.000000000',\n       '2023-04-06T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (10)precipitation(time, lon, lat)float32dask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;units :mm/daylong_name :Daily mean precipitation rate (combined microwave-IR) estimate. Formerly precipitationCal.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.36 GiB\n24.72 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nprecipitation_cnt\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of all valid half-hourly precipitation retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nprecipitation_cnt_cond\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of half-hourly precipitation retrievals for the day where precipitation is at least 0.01 mm/hr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation\n\n\n(time, lon, lat)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm/day\n\nlong_name :\n\nDaily mean High Quality precipitation rate from all available microwave sources. Formerly HQprecipitation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.36 GiB\n24.72 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation_cnt\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of all valid half-hourly MWprecipitation retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nMWprecipitation_cnt_cond\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of half-hourly MWprecipitation retrievals for the day where precipitation is at least 0.01 mm/hr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nrandomError\n\n\n(time, lon, lat)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm/day\n\nlong_name :\n\nRoot-mean-square error estimate for combined microwave-IR daily precipitation rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.36 GiB\n24.72 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nrandomError_cnt\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ncount\n\nlong_name :\n\nCount of valid half-hourly randomError retrievals for the day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\nprobabilityLiquidPrecipitation\n\n\n(time, lon, lat)\n\n\nint8\n\n\ndask.array&lt;chunksize=(1, 3600, 1800), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\npercent\n\nlong_name :\n\nProbability of liquid precipitation\n\ndescription :\n\nProbability of liquid precipitation estimated with a diagnostic parameterization using ancillary data. 0=missing values; 1=likely solid; 100=likely liquid or no precipitation. Screen by positive precipitation or precipitation_cnt_cond to locate meaningful probabilities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n858.99 MiB\n6.18 MiB\n\n\nShape\n(139, 3600, 1800)\n(1, 3600, 1800)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntime_bnds\n\n\n(time, nv)\n\n\ndatetime64[ns]\n\n\ndask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\n\n\n\n\ncoordinates :\n\ntime nv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.17 kiB\n16 B\n\n\nShape\n(139, 2)\n(1, 2)\n\n\nDask graph\n139 chunks in 279 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)lonPandasIndexPandasIndex(Float64Index([ -179.9499969482422, -179.85000610351562,             -179.75,\n              -179.64999389648438,  -179.5500030517578,  -179.4499969482422,\n              -179.35000610351562,             -179.25, -179.14999389648438,\n               -179.0500030517578,\n              ...\n                179.0500030517578,  179.14999389648438,              179.25,\n               179.35000610351562,   179.4499969482422,   179.5500030517578,\n               179.64999389648438,              179.75,  179.85000610351562,\n                179.9499969482422],\n             dtype='float64', name='lon', length=3600))latPandasIndexPandasIndex(Float64Index([            -89.95, -89.85000000000001,             -89.75,\n                          -89.65,             -89.55,             -89.45,\n              -89.35000000000001,             -89.25,             -89.15,\n                          -89.05,\n              ...\n                           89.05,  89.15000000000002,  89.25000000000001,\n               89.35000000000001,              89.45,              89.55,\n               89.65000000000002,  89.75000000000001,  89.85000000000001,\n                           89.95],\n             dtype='float64', name='lat', length=1800))timePandasIndexPandasIndex(DatetimeIndex(['2022-11-19', '2022-11-20', '2022-11-21', '2022-11-22',\n               '2022-11-23', '2022-11-24', '2022-11-25', '2022-11-26',\n               '2022-11-27', '2022-11-28',\n               ...\n               '2023-03-28', '2023-03-29', '2023-03-30', '2023-03-31',\n               '2023-04-01', '2023-04-02', '2023-04-03', '2023-04-04',\n               '2023-04-05', '2023-04-06'],\n              dtype='datetime64[ns]', name='time', length=139, freq=None))Attributes: (9)BeginDate :2022-11-19BeginTime :00:00:00.000ZEndDate :2022-11-19EndTime :23:59:59.999ZFileHeader :StartGranuleDateTime=2022-11-19T00:00:00.000Z;\nStopGranuleDateTime=2022-11-19T23:59:59.999ZInputPointer :3B-HHR.MS.MRG.3IMERG.20221119-S000000-E002959.0000.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S003000-E005959.0030.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S010000-E012959.0060.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S013000-E015959.0090.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S020000-E022959.0120.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S023000-E025959.0150.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S030000-E032959.0180.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S033000-E035959.0210.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S040000-E042959.0240.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S043000-E045959.0270.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S050000-E052959.0300.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S053000-E055959.0330.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S060000-E062959.0360.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S063000-E065959.0390.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S070000-E072959.0420.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S073000-E075959.0450.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S080000-E082959.0480.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S083000-E085959.0510.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S090000-E092959.0540.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S093000-E095959.0570.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S100000-E102959.0600.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S103000-E105959.0630.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S110000-E112959.0660.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S113000-E115959.0690.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S120000-E122959.0720.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S123000-E125959.0750.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S130000-E132959.0780.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S133000-E135959.0810.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S140000-E142959.0840.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S143000-E145959.0870.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S150000-E152959.0900.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S153000-E155959.0930.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S160000-E162959.0960.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S163000-E165959.0990.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S170000-E172959.1020.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S173000-E175959.1050.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S180000-E182959.1080.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S183000-E185959.1110.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S190000-E192959.1140.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S193000-E195959.1170.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S200000-E202959.1200.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S203000-E205959.1230.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S210000-E212959.1260.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S213000-E215959.1290.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S220000-E222959.1320.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S223000-E225959.1350.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S230000-E232959.1380.V07A.HDF5;3B-HHR.MS.MRG.3IMERG.20221119-S233000-E235959.1410.V07A.HDF5title :GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 degree (GPM_3IMERGDF)DOI :10.5067/GPM/IMERGDF/DAY/07ProductionTime :2023-08-25T14:03:25.792Z"
  },
  {
    "objectID": "tutorials/Earthdata_Search_Discovery_earthaccess.html#resources",
    "href": "tutorials/Earthdata_Search_Discovery_earthaccess.html#resources",
    "title": "Data discovery with earthaccess",
    "section": "Resources",
    "text": "Resources\n\nNASA’s Common Metadata Repository (CMR) API\n\nearthaccess repository\nearthaccess documentation\nEarthdata Search"
  }
]